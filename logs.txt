* 
* ==> Audit <==
* |-----------|-----------------------|----------|---------------------|---------|---------------------|---------------------|
|  Command  |         Args          | Profile  |        User         | Version |     Start Time      |      End Time       |
|-----------|-----------------------|----------|---------------------|---------|---------------------|---------------------|
| start     |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 21:40 WIB | 28 Aug 23 21:45 WIB |
| start     |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 21:47 WIB | 28 Aug 23 21:48 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 22:53 WIB | 28 Aug 23 22:53 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 22:57 WIB | 28 Aug 23 22:57 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 23:18 WIB | 28 Aug 23 23:18 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 23:34 WIB | 28 Aug 23 23:34 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 23:35 WIB | 28 Aug 23 23:35 WIB |
| stop      |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 23:47 WIB | 28 Aug 23 23:47 WIB |
| start     |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 23:47 WIB | 28 Aug 23 23:48 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 28 Aug 23 23:50 WIB | 28 Aug 23 23:50 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:10 WIB | 29 Aug 23 00:10 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:18 WIB | 29 Aug 23 00:18 WIB |
| start     | --driver=virtualbox   | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:25 WIB |                     |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:30 WIB | 29 Aug 23 00:30 WIB |
| start     | --driver=virtualbox   | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:32 WIB |                     |
| start     | --driver=docker       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:32 WIB |                     |
| delete    |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:39 WIB | 29 Aug 23 00:39 WIB |
| start     | --driver=virtualbox   | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:39 WIB |                     |
| start     |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:44 WIB |                     |
| start     |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 00:50 WIB |                     |
| config    | set driver podman     | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:48 WIB | 29 Aug 23 08:48 WIB |
| start     |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:48 WIB |                     |
| delete    |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:48 WIB | 29 Aug 23 08:49 WIB |
| config    | set driver podman     | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:49 WIB | 29 Aug 23 08:49 WIB |
| start     | --driver=podman       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:49 WIB | 29 Aug 23 08:53 WIB |
| config    | set rootless true     | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:51 WIB | 29 Aug 23 08:51 WIB |
| ip        |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:55 WIB | 29 Aug 23 08:56 WIB |
| kubectl   | -- get po -A          | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 08:59 WIB | 29 Aug 23 09:00 WIB |
| dashboard |                       | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 09:00 WIB |                     |
| addons    | enable metrics-server | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 09:02 WIB | 29 Aug 23 09:03 WIB |
| service   | karsajobs             | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 09:31 WIB | 29 Aug 23 09:36 WIB |
| service   | karsajobs             | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 11:34 WIB |                     |
| service   | karsajobs             | minikube | afridermawanginting | v1.31.2 | 29 Aug 23 11:35 WIB |                     |
|-----------|-----------------------|----------|---------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/08/29 08:49:48
Running on machine: Afris-MacBook-Pro
Binary: Built with gc go1.21.0 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0829 08:49:48.852740   43612 out.go:296] Setting OutFile to fd 1 ...
I0829 08:49:48.853795   43612 out.go:348] isatty.IsTerminal(1) = true
I0829 08:49:48.853803   43612 out.go:309] Setting ErrFile to fd 2...
I0829 08:49:48.853815   43612 out.go:348] isatty.IsTerminal(2) = true
I0829 08:49:48.854509   43612 root.go:338] Updating PATH: /Users/afridermawanginting/.minikube/bin
I0829 08:49:48.857098   43612 out.go:303] Setting JSON to false
I0829 08:49:48.898053   43612 start.go:128] hostinfo: {"hostname":"Afris-MacBook-Pro.local","uptime":50158,"bootTime":1693223630,"procs":500,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.5","kernelVersion":"22.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"7e672515-e0c8-5358-8e6f-a18b544cd7c1"}
W0829 08:49:48.898218   43612 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0829 08:49:48.906013   43612 out.go:177] 😄  minikube v1.31.2 on Darwin 13.5
I0829 08:49:48.914296   43612 notify.go:220] Checking for updates...
I0829 08:49:48.915514   43612 driver.go:373] Setting default libvirt URI to qemu:///system
I0829 08:49:49.786730   43612 podman.go:123] podman version: 4.6.1
I0829 08:49:49.790903   43612 out.go:177] ✨  Using the podman (experimental) driver based on user configuration
I0829 08:49:49.796929   43612 start.go:298] selected driver: podman
I0829 08:49:49.796944   43612 start.go:902] validating driver "podman" against <nil>
I0829 08:49:49.796956   43612 start.go:913] status for podman: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0829 08:49:49.797239   43612 cli_runner.go:164] Run: podman system info --format json
I0829 08:49:50.140966   43612 info.go:288] podman info: {Host:{BuildahVersion:1.31.2 CgroupVersion:v2 Conmon:{Package:conmon-2.1.7-2.fc38.x86_64 Path:/usr/bin/conmon Version:conmon version 2.1.7, commit: } Distribution:{Distribution:fedora Version:38} MemFree:1260740608 MemTotal:2048446464 OCIRuntime:{Name:crun Package:crun-1.8.6-1.fc38.x86_64 Path:/usr/bin/crun Version:crun version 1.8.6
commit: 73f759f4a39769f60990e7d225f561b4f4f06bcf
rundir: /run/crun
spec: 1.0.0
+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL} SwapFree:0 SwapTotal:0 Arch:amd64 Cpus:2 Eventlogger:journald Hostname:localhost.localdomain Kernel:6.4.11-200.fc38.x86_64 Os:linux Security:{Rootless:false} Uptime:0h 5m 0.00s} Registries:{Search:[docker.io]} Store:{ConfigFile:/usr/share/containers/storage.conf ContainerStore:{Number:0} GraphDriverName:overlay GraphOptions:{} GraphRoot:/var/lib/containers/storage GraphStatus:{BackingFilesystem:xfs NativeOverlayDiff:false SupportsDType:true UsingMetacopy:true} ImageStore:{Number:0} RunRoot:/run/containers/storage VolumePath:/var/lib/containers/storage/volumes}}
I0829 08:49:50.141139   43612 start_flags.go:305] no existing cluster config was found, will generate one from the flags 
I0829 08:49:50.141723   43612 start_flags.go:382] Using suggested 1953MB memory alloc based on sys=8192MB, container=1953MB
I0829 08:49:50.143401   43612 start_flags.go:901] Wait components to verify : map[apiserver:true system_pods:true]
I0829 08:49:50.148072   43612 out.go:177] 📌  Using Podman driver with root privileges
I0829 08:49:50.153771   43612 cni.go:84] Creating CNI manager for ""
I0829 08:49:50.154234   43612 cni.go:158] "podman" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0829 08:49:50.154268   43612 start_flags.go:314] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0829 08:49:50.154279   43612 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:1953 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0829 08:49:50.158242   43612 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0829 08:49:50.167449   43612 cache.go:122] Beginning downloading kic base image for podman with docker
I0829 08:49:50.172902   43612 out.go:177] 🚜  Pulling base image ...
I0829 08:49:50.180956   43612 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0829 08:49:50.181021   43612 preload.go:148] Found local preload: /Users/afridermawanginting/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I0829 08:49:50.181018   43612 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I0829 08:49:50.181031   43612 cache.go:57] Caching tarball of preloaded images
I0829 08:49:50.181832   43612 preload.go:174] Found /Users/afridermawanginting/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0829 08:49:50.181848   43612 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I0829 08:49:50.182398   43612 profile.go:148] Saving config to /Users/afridermawanginting/.minikube/profiles/minikube/config.json ...
I0829 08:49:50.182507   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.minikube/profiles/minikube/config.json: {Name:mkbc16e4958276a7924dc753168f9e6c571e2fef Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:49:50.182977   43612 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory
I0829 08:49:50.183313   43612 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory, skipping pull
I0829 08:49:50.183364   43612 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in cache, skipping pull
I0829 08:49:50.183391   43612 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 as a tarball
E0829 08:49:50.183461   43612 cache.go:190] Error downloading kic artifacts:  not yet implemented, see issue #8426
I0829 08:49:50.183523   43612 cache.go:195] Successfully downloaded all kic artifacts
I0829 08:49:50.184239   43612 start.go:365] acquiring machines lock for minikube: {Name:mk79614a5050276520c73b32cb8ba65523bba7df Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0829 08:49:50.185446   43612 start.go:369] acquired machines lock for "minikube" in 1.060491ms
I0829 08:49:50.186281   43612 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:1953 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0} &{Name: IP: Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0829 08:49:50.187152   43612 start.go:125] createHost starting for "" (driver="podman")
I0829 08:49:50.192826   43612 out.go:204] 🔥  Creating podman container (CPUs=2, Memory=1953MB) ...
I0829 08:49:50.193484   43612 start.go:159] libmachine.API.Create for "minikube" (driver="podman")
I0829 08:49:50.193563   43612 client.go:168] LocalClient.Create starting
I0829 08:49:50.194958   43612 main.go:141] libmachine: Reading certificate data from /Users/afridermawanginting/.minikube/certs/ca.pem
I0829 08:49:50.195688   43612 main.go:141] libmachine: Decoding PEM data...
I0829 08:49:50.195792   43612 main.go:141] libmachine: Parsing certificate...
I0829 08:49:50.196850   43612 main.go:141] libmachine: Reading certificate data from /Users/afridermawanginting/.minikube/certs/cert.pem
I0829 08:49:50.197170   43612 main.go:141] libmachine: Decoding PEM data...
I0829 08:49:50.197186   43612 main.go:141] libmachine: Parsing certificate...
I0829 08:49:50.198709   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:49:50.529532   43612 cli_runner.go:164] Run: podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
W0829 08:49:50.874078   43612 cli_runner.go:211] podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}" returned with exit code 125
I0829 08:49:50.876192   43612 network_create.go:281] running [podman network inspect minikube] to gather additional debugging logs...
I0829 08:49:50.876240   43612 cli_runner.go:164] Run: podman network inspect minikube
W0829 08:49:51.103179   43612 cli_runner.go:211] podman network inspect minikube returned with exit code 125
I0829 08:49:51.103231   43612 network_create.go:284] error running [podman network inspect minikube]: podman network inspect minikube: exit status 125
stdout:
[]

stderr:
Error: network minikube: network not found
I0829 08:49:51.103243   43612 network_create.go:286] output of [podman network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: network minikube: network not found

** /stderr **
I0829 08:49:51.103473   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:49:51.402290   43612 cli_runner.go:164] Run: podman network inspect podman --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
I0829 08:49:51.618473   43612 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000e34550}
I0829 08:49:51.618514   43612 network_create.go:123] attempt to create podman network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 0 ...
I0829 08:49:51.618667   43612 cli_runner.go:164] Run: podman network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0829 08:49:51.819971   43612 network_create.go:107] podman network minikube 192.168.49.0/24 created
I0829 08:49:51.845083   43612 kic.go:117] calculated static IP "192.168.49.2" for the "minikube" container
I0829 08:49:51.845306   43612 cli_runner.go:164] Run: podman ps -a --format {{.Names}}
I0829 08:49:52.076258   43612 cli_runner.go:164] Run: podman volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0829 08:49:52.321514   43612 oci.go:103] Successfully created a podman volume minikube
I0829 08:49:52.321725   43612 cli_runner.go:164] Run: podman run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40 -d /var/lib
I0829 08:51:52.364644   43612 cli_runner.go:217] Completed: podman run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40 -d /var/lib: (2m0.039454992s)
I0829 08:51:52.364750   43612 oci.go:107] Successfully prepared a podman volume minikube
I0829 08:51:52.364774   43612 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0829 08:51:52.364868   43612 kic.go:190] Starting extracting preloaded images to volume ...
I0829 08:51:52.365453   43612 cli_runner.go:164] Run: podman run --rm --entrypoint /usr/bin/tar -v /Users/afridermawanginting/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40 -I lz4 -xf /preloaded.tar -C /extractDir
I0829 08:52:24.172949   43612 cli_runner.go:217] Completed: podman run --rm --entrypoint /usr/bin/tar -v /Users/afridermawanginting/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40 -I lz4 -xf /preloaded.tar -C /extractDir: (31.80620117s)
I0829 08:52:24.173111   43612 kic.go:199] duration metric: took 31.807353 seconds to extract preloaded images to volume
I0829 08:52:24.176283   43612 cli_runner.go:164] Run: podman info --format "'{{json .SecurityOptions}}'"
W0829 08:52:24.742560   43612 cli_runner.go:211] podman info --format "'{{json .SecurityOptions}}'" returned with exit code 125
I0829 08:52:24.748092   43612 cli_runner.go:164] Run: podman run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var:exec --memory-swap=1953mb --memory=1953mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40
I0829 08:52:36.893314   43612 cli_runner.go:217] Completed: podman run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var:exec --memory-swap=1953mb --memory=1953mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40: (12.14472174s)
I0829 08:52:36.894000   43612 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Running}}
I0829 08:52:37.146634   43612 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0829 08:52:37.388858   43612 cli_runner.go:164] Run: podman exec minikube stat /var/lib/dpkg/alternatives/iptables
I0829 08:52:37.768680   43612 oci.go:144] the created container "minikube" has a running status.
I0829 08:52:37.778742   43612 kic.go:221] Creating ssh key for kic: /Users/afridermawanginting/.minikube/machines/minikube/id_rsa...
I0829 08:52:38.232356   43612 kic_runner.go:191] podman (temp): /Users/afridermawanginting/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0829 08:52:38.236466   43612 kic_runner.go:276] Run: /usr/local/bin/podman exec -i minikube tee /home/docker/.ssh/authorized_keys
I0829 08:52:38.586030   43612 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0829 08:52:39.064220   43612 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0829 08:52:39.064245   43612 kic_runner.go:114] Args: [podman exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0829 08:52:40.119209   43612 kic_runner.go:123] Done: [podman exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (1.054862s)
I0829 08:52:40.119642   43612 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0829 08:52:40.676218   43612 machine.go:88] provisioning docker machine ...
I0829 08:52:40.683225   43612 ubuntu.go:169] provisioning hostname "minikube"
I0829 08:52:40.683846   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:41.229407   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:41.588939   43612 main.go:141] libmachine: Using SSH client type: native
I0829 08:52:41.590183   43612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f4ae0] 0x1003f77c0 <nil>  [] 0s} 127.0.0.1 33227 <nil> <nil>}
I0829 08:52:41.590216   43612 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0829 08:52:42.209057   43612 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0829 08:52:42.209471   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:42.585126   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:43.096750   43612 main.go:141] libmachine: Using SSH client type: native
I0829 08:52:43.097326   43612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f4ae0] 0x1003f77c0 <nil>  [] 0s} 127.0.0.1 33227 <nil> <nil>}
I0829 08:52:43.097342   43612 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0829 08:52:43.405316   43612 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0829 08:52:43.405346   43612 ubuntu.go:175] set auth options {CertDir:/Users/afridermawanginting/.minikube CaCertPath:/Users/afridermawanginting/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/afridermawanginting/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/afridermawanginting/.minikube/machines/server.pem ServerKeyPath:/Users/afridermawanginting/.minikube/machines/server-key.pem ClientKeyPath:/Users/afridermawanginting/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/afridermawanginting/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/afridermawanginting/.minikube}
I0829 08:52:43.405386   43612 ubuntu.go:177] setting up certificates
I0829 08:52:43.410735   43612 provision.go:83] configureAuth start
I0829 08:52:43.410902   43612 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I0829 08:52:43.709368   43612 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0829 08:52:44.141283   43612 provision.go:138] copyHostCerts
I0829 08:52:44.142708   43612 exec_runner.go:144] found /Users/afridermawanginting/.minikube/key.pem, removing ...
I0829 08:52:44.142725   43612 exec_runner.go:203] rm: /Users/afridermawanginting/.minikube/key.pem
I0829 08:52:44.143528   43612 exec_runner.go:151] cp: /Users/afridermawanginting/.minikube/certs/key.pem --> /Users/afridermawanginting/.minikube/key.pem (1675 bytes)
I0829 08:52:44.159764   43612 exec_runner.go:144] found /Users/afridermawanginting/.minikube/ca.pem, removing ...
I0829 08:52:44.159775   43612 exec_runner.go:203] rm: /Users/afridermawanginting/.minikube/ca.pem
I0829 08:52:44.159928   43612 exec_runner.go:151] cp: /Users/afridermawanginting/.minikube/certs/ca.pem --> /Users/afridermawanginting/.minikube/ca.pem (1111 bytes)
I0829 08:52:44.160763   43612 exec_runner.go:144] found /Users/afridermawanginting/.minikube/cert.pem, removing ...
I0829 08:52:44.160773   43612 exec_runner.go:203] rm: /Users/afridermawanginting/.minikube/cert.pem
I0829 08:52:44.160916   43612 exec_runner.go:151] cp: /Users/afridermawanginting/.minikube/certs/cert.pem --> /Users/afridermawanginting/.minikube/cert.pem (1155 bytes)
I0829 08:52:44.161676   43612 provision.go:112] generating server cert: /Users/afridermawanginting/.minikube/machines/server.pem ca-key=/Users/afridermawanginting/.minikube/certs/ca.pem private-key=/Users/afridermawanginting/.minikube/certs/ca-key.pem org=afridermawanginting.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0829 08:52:44.341518   43612 provision.go:172] copyRemoteCerts
I0829 08:52:44.342328   43612 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0829 08:52:44.342457   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:44.917337   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:45.479233   43612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:33227 SSHKeyPath:/Users/afridermawanginting/.minikube/machines/minikube/id_rsa Username:docker}
I0829 08:52:45.613550   43612 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.271139999s)
I0829 08:52:45.614836   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1111 bytes)
I0829 08:52:45.697013   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/machines/server.pem --> /etc/docker/server.pem (1237 bytes)
I0829 08:52:45.736438   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0829 08:52:45.783522   43612 provision.go:86] duration metric: configureAuth took 2.372675276s
I0829 08:52:45.783559   43612 ubuntu.go:193] setting minikube options for container-runtime
I0829 08:52:45.784636   43612 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0829 08:52:45.784897   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:46.102287   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:46.308520   43612 main.go:141] libmachine: Using SSH client type: native
I0829 08:52:46.309052   43612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f4ae0] 0x1003f77c0 <nil>  [] 0s} 127.0.0.1 33227 <nil> <nil>}
I0829 08:52:46.309071   43612 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0829 08:52:46.453843   43612 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0829 08:52:46.453856   43612 ubuntu.go:71] root file system type: overlay
I0829 08:52:46.454018   43612 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0829 08:52:46.454155   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:46.737227   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:46.955540   43612 main.go:141] libmachine: Using SSH client type: native
I0829 08:52:46.956015   43612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f4ae0] 0x1003f77c0 <nil>  [] 0s} 127.0.0.1 33227 <nil> <nil>}
I0829 08:52:46.956129   43612 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0829 08:52:47.128860   43612 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0829 08:52:47.129535   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:47.446464   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:47.626819   43612 main.go:141] libmachine: Using SSH client type: native
I0829 08:52:47.627196   43612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f4ae0] 0x1003f77c0 <nil>  [] 0s} 127.0.0.1 33227 <nil> <nil>}
I0829 08:52:47.627217   43612 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0829 08:52:49.069175   43612 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-07-07 14:50:55.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-08-29 01:52:47.273498333 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0829 08:52:49.069222   43612 machine.go:91] provisioned docker machine in 8.392738963s
I0829 08:52:49.069229   43612 client.go:171] LocalClient.Create took 2m58.87065846s
I0829 08:52:49.069261   43612 start.go:167] duration metric: libmachine.API.Create for "minikube" took 2m58.870792593s
I0829 08:52:49.069815   43612 start.go:300] post-start starting for "minikube" (driver="podman")
I0829 08:52:49.070009   43612 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0829 08:52:49.070169   43612 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0829 08:52:49.070257   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:49.416230   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:49.611628   43612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:33227 SSHKeyPath:/Users/afridermawanginting/.minikube/machines/minikube/id_rsa Username:docker}
I0829 08:52:49.690772   43612 ssh_runner.go:195] Run: cat /etc/os-release
I0829 08:52:49.697066   43612 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0829 08:52:49.697115   43612 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0829 08:52:49.697131   43612 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0829 08:52:49.697140   43612 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0829 08:52:49.697839   43612 filesync.go:126] Scanning /Users/afridermawanginting/.minikube/addons for local assets ...
I0829 08:52:49.698269   43612 filesync.go:126] Scanning /Users/afridermawanginting/.minikube/files for local assets ...
I0829 08:52:49.698429   43612 start.go:303] post-start completed in 628.588755ms
I0829 08:52:49.699103   43612 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I0829 08:52:49.892685   43612 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0829 08:52:50.069702   43612 profile.go:148] Saving config to /Users/afridermawanginting/.minikube/profiles/minikube/config.json ...
I0829 08:52:50.071319   43612 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0829 08:52:50.071415   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:50.390299   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:50.575443   43612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:33227 SSHKeyPath:/Users/afridermawanginting/.minikube/machines/minikube/id_rsa Username:docker}
I0829 08:52:50.653247   43612 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0829 08:52:50.662035   43612 start.go:128] duration metric: createHost completed in 3m0.469792359s
I0829 08:52:50.662057   43612 start.go:83] releasing machines lock for "minikube", held for 3m0.471541024s
I0829 08:52:50.662554   43612 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I0829 08:52:50.858998   43612 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0829 08:52:51.133759   43612 ssh_runner.go:195] Run: cat /version.json
I0829 08:52:51.133913   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:51.136050   43612 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0829 08:52:51.137981   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:51.509068   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:51.552233   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:52:51.762743   43612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:33227 SSHKeyPath:/Users/afridermawanginting/.minikube/machines/minikube/id_rsa Username:docker}
I0829 08:52:51.824653   43612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:33227 SSHKeyPath:/Users/afridermawanginting/.minikube/machines/minikube/id_rsa Username:docker}
I0829 08:52:52.467084   43612 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.330746083s)
I0829 08:52:52.467140   43612 ssh_runner.go:235] Completed: cat /version.json: (1.333322886s)
I0829 08:52:52.469061   43612 ssh_runner.go:195] Run: systemctl --version
I0829 08:52:52.477936   43612 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0829 08:52:52.486453   43612 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0829 08:52:52.538598   43612 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0829 08:52:52.538842   43612 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0829 08:52:52.580578   43612 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0829 08:52:52.580593   43612 start.go:466] detecting cgroup driver to use...
I0829 08:52:52.580608   43612 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0829 08:52:52.582629   43612 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0829 08:52:52.618161   43612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0829 08:52:52.635203   43612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0829 08:52:52.652872   43612 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0829 08:52:52.653014   43612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0829 08:52:52.671741   43612 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0829 08:52:52.691136   43612 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0829 08:52:52.714687   43612 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0829 08:52:52.731857   43612 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0829 08:52:52.746289   43612 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0829 08:52:52.763472   43612 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0829 08:52:52.778444   43612 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0829 08:52:52.798103   43612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0829 08:52:53.019587   43612 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0829 08:52:53.165560   43612 start.go:466] detecting cgroup driver to use...
I0829 08:52:53.165582   43612 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0829 08:52:53.166148   43612 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0829 08:52:53.205664   43612 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0829 08:52:53.205839   43612 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0829 08:52:53.227999   43612 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0829 08:52:53.282391   43612 ssh_runner.go:195] Run: which cri-dockerd
I0829 08:52:53.293560   43612 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0829 08:52:53.329761   43612 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0829 08:52:53.378972   43612 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0829 08:52:53.722866   43612 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0829 08:52:53.965458   43612 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0829 08:52:53.965484   43612 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0829 08:52:54.028561   43612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0829 08:52:54.157073   43612 ssh_runner.go:195] Run: sudo systemctl restart docker
I0829 08:52:54.602898   43612 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0829 08:52:54.734245   43612 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0829 08:52:54.851117   43612 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0829 08:52:54.980271   43612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0829 08:52:55.107659   43612 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0829 08:52:55.166093   43612 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0829 08:52:55.301248   43612 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0829 08:52:55.753279   43612 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0829 08:52:55.753772   43612 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0829 08:52:55.763138   43612 start.go:534] Will wait 60s for crictl version
I0829 08:52:55.763307   43612 ssh_runner.go:195] Run: which crictl
I0829 08:52:55.770233   43612 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0829 08:52:56.064729   43612 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0829 08:52:56.064926   43612 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0829 08:52:56.581382   43612 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0829 08:52:56.633191   43612 out.go:204] 🐳  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
E0829 08:52:56.633293   43612 start.go:131] Unable to get host IP: RoutableHostIPFromInside is currently only implemented for linux
I0829 08:52:56.633598   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:52:56.949490   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0829 08:52:57.150707   43612 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0829 08:52:57.150921   43612 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0829 08:52:57.184478   43612 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0829 08:52:57.185050   43612 docker.go:566] Images already preloaded, skipping extraction
I0829 08:52:57.185549   43612 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0829 08:52:57.221953   43612 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0829 08:52:57.221987   43612 cache_images.go:84] Images are preloaded, skipping loading
I0829 08:52:57.222783   43612 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0829 08:52:58.059713   43612 cni.go:84] Creating CNI manager for ""
I0829 08:52:58.059730   43612 cni.go:158] "podman" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0829 08:52:58.060456   43612 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0829 08:52:58.060490   43612 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0829 08:52:58.060732   43612 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0829 08:52:58.061124   43612 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0829 08:52:58.061298   43612 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I0829 08:52:58.115227   43612 binaries.go:44] Found k8s binaries, skipping transfer
I0829 08:52:58.115468   43612 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0829 08:52:58.134352   43612 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0829 08:52:58.163060   43612 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0829 08:52:58.193864   43612 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0829 08:52:58.224254   43612 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0829 08:52:58.234482   43612 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0829 08:52:58.259226   43612 certs.go:56] Setting up /Users/afridermawanginting/.minikube/profiles/minikube for IP: 192.168.49.2
I0829 08:52:58.259315   43612 certs.go:190] acquiring lock for shared ca certs: {Name:mk39e5bacc41c46c63e9e7745c009dd500eeed13 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:52:58.260509   43612 certs.go:199] skipping minikubeCA CA generation: /Users/afridermawanginting/.minikube/ca.key
I0829 08:52:58.261246   43612 certs.go:199] skipping proxyClientCA CA generation: /Users/afridermawanginting/.minikube/proxy-client-ca.key
I0829 08:52:58.261831   43612 certs.go:319] generating minikube-user signed cert: /Users/afridermawanginting/.minikube/profiles/minikube/client.key
I0829 08:52:58.261903   43612 crypto.go:68] Generating cert /Users/afridermawanginting/.minikube/profiles/minikube/client.crt with IP's: []
I0829 08:52:58.500380   43612 crypto.go:156] Writing cert to /Users/afridermawanginting/.minikube/profiles/minikube/client.crt ...
I0829 08:52:58.500396   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.minikube/profiles/minikube/client.crt: {Name:mkfbd75815e7b38a9fcf5cae9f176f8281bd6963 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:52:58.500810   43612 crypto.go:164] Writing key to /Users/afridermawanginting/.minikube/profiles/minikube/client.key ...
I0829 08:52:58.500821   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.minikube/profiles/minikube/client.key: {Name:mkadb930db3c2e5bdc7d65dda610f2eba2da1434 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:52:58.501137   43612 certs.go:319] generating minikube signed cert: /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0829 08:52:58.501739   43612 crypto.go:68] Generating cert /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0829 08:52:58.705272   43612 crypto.go:156] Writing cert to /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I0829 08:52:58.705285   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk767d54185f82c6f164269bd01bb65379fa3ba0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:52:58.705840   43612 crypto.go:164] Writing key to /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I0829 08:52:58.705851   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk75c15afd8a95a467d9c4027ce2cfea9d13c626 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:52:58.706258   43612 certs.go:337] copying /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.crt
I0829 08:52:58.707315   43612 certs.go:341] copying /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.key
I0829 08:52:58.707535   43612 certs.go:319] generating aggregator signed cert: /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.key
I0829 08:52:58.707547   43612 crypto.go:68] Generating cert /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0829 08:52:58.880285   43612 crypto.go:156] Writing cert to /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.crt ...
I0829 08:52:58.880299   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.crt: {Name:mk4431b7861c72993c2de08ca5a9aebe60687800 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:52:58.881987   43612 crypto.go:164] Writing key to /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.key ...
I0829 08:52:58.882008   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.key: {Name:mk02c88c0ce76b502217831d8f63be8e97f5a60b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:52:58.884658   43612 certs.go:437] found cert: /Users/afridermawanginting/.minikube/certs/Users/afridermawanginting/.minikube/certs/ca-key.pem (1675 bytes)
I0829 08:52:58.885156   43612 certs.go:437] found cert: /Users/afridermawanginting/.minikube/certs/Users/afridermawanginting/.minikube/certs/ca.pem (1111 bytes)
I0829 08:52:58.885709   43612 certs.go:437] found cert: /Users/afridermawanginting/.minikube/certs/Users/afridermawanginting/.minikube/certs/cert.pem (1155 bytes)
I0829 08:52:58.886206   43612 certs.go:437] found cert: /Users/afridermawanginting/.minikube/certs/Users/afridermawanginting/.minikube/certs/key.pem (1675 bytes)
I0829 08:52:58.893094   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0829 08:52:58.937286   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0829 08:52:58.981187   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0829 08:52:59.022478   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0829 08:52:59.060955   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0829 08:52:59.100734   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0829 08:52:59.140867   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0829 08:52:59.180146   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0829 08:52:59.218524   43612 ssh_runner.go:362] scp /Users/afridermawanginting/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0829 08:52:59.262557   43612 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0829 08:52:59.296072   43612 ssh_runner.go:195] Run: openssl version
I0829 08:52:59.335328   43612 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0829 08:52:59.358500   43612 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0829 08:52:59.367259   43612 certs.go:480] hashing: -rw-r--r--. 1 root root 1111 Aug 28 14:45 /usr/share/ca-certificates/minikubeCA.pem
I0829 08:52:59.367389   43612 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0829 08:52:59.381093   43612 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0829 08:52:59.397211   43612 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0829 08:52:59.403096   43612 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0829 08:52:59.403162   43612 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:1953 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0829 08:52:59.403305   43612 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0829 08:52:59.437252   43612 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0829 08:52:59.453040   43612 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0829 08:52:59.469269   43612 kubeadm.go:226] ignoring SystemVerification for kubeadm because of podman driver
I0829 08:52:59.469383   43612 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0829 08:52:59.485026   43612 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0829 08:52:59.485054   43612 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0829 08:53:00.270906   43612 kubeadm.go:322] [init] Using Kubernetes version: v1.27.4
I0829 08:53:00.270957   43612 kubeadm.go:322] [preflight] Running pre-flight checks
I0829 08:53:02.016912   43612 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0829 08:53:02.017044   43612 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0829 08:53:02.017172   43612 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0829 08:53:02.626903   43612 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0829 08:53:02.634050   43612 out.go:204]     ▪ Generating certificates and keys ...
I0829 08:53:02.634286   43612 kubeadm.go:322] [certs] Using existing ca certificate authority
I0829 08:53:02.634377   43612 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0829 08:53:02.888998   43612 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0829 08:53:03.174076   43612 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0829 08:53:03.397589   43612 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0829 08:53:03.452352   43612 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0829 08:53:03.775109   43612 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0829 08:53:03.775900   43612 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0829 08:53:03.965345   43612 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0829 08:53:03.965523   43612 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0829 08:53:04.390872   43612 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0829 08:53:05.172030   43612 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0829 08:53:05.369994   43612 kubeadm.go:322] [certs] Generating "sa" key and public key
I0829 08:53:05.372479   43612 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0829 08:53:05.510479   43612 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0829 08:53:05.724244   43612 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0829 08:53:05.949575   43612 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0829 08:53:06.039392   43612 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0829 08:53:06.057198   43612 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0829 08:53:06.058478   43612 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0829 08:53:06.058548   43612 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0829 08:53:06.217788   43612 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0829 08:53:06.224870   43612 out.go:204]     ▪ Booting up control plane ...
I0829 08:53:06.225056   43612 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0829 08:53:06.225196   43612 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0829 08:53:06.225272   43612 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0829 08:53:06.225396   43612 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0829 08:53:06.259442   43612 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0829 08:53:14.771954   43612 kubeadm.go:322] [apiclient] All control plane components are healthy after 8.511653 seconds
I0829 08:53:14.772107   43612 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0829 08:53:14.793366   43612 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0829 08:53:15.327660   43612 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0829 08:53:15.327944   43612 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0829 08:53:15.843757   43612 kubeadm.go:322] [bootstrap-token] Using token: r5liuw.m9dedii248cqkq0q
I0829 08:53:15.857047   43612 out.go:204]     ▪ Configuring RBAC rules ...
I0829 08:53:15.857311   43612 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0829 08:53:15.867096   43612 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0829 08:53:15.880230   43612 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0829 08:53:15.888186   43612 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0829 08:53:15.895902   43612 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0829 08:53:15.905819   43612 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0829 08:53:15.928020   43612 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0829 08:53:16.222416   43612 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0829 08:53:16.274125   43612 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0829 08:53:16.275639   43612 kubeadm.go:322] 
I0829 08:53:16.275728   43612 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0829 08:53:16.275736   43612 kubeadm.go:322] 
I0829 08:53:16.275841   43612 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0829 08:53:16.275849   43612 kubeadm.go:322] 
I0829 08:53:16.275891   43612 kubeadm.go:322]   mkdir -p $HOME/.kube
I0829 08:53:16.275974   43612 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0829 08:53:16.276067   43612 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0829 08:53:16.276075   43612 kubeadm.go:322] 
I0829 08:53:16.276224   43612 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0829 08:53:16.276234   43612 kubeadm.go:322] 
I0829 08:53:16.276343   43612 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0829 08:53:16.276367   43612 kubeadm.go:322] 
I0829 08:53:16.276547   43612 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0829 08:53:16.276740   43612 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0829 08:53:16.277027   43612 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0829 08:53:16.277034   43612 kubeadm.go:322] 
I0829 08:53:16.277195   43612 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0829 08:53:16.277388   43612 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0829 08:53:16.277398   43612 kubeadm.go:322] 
I0829 08:53:16.277571   43612 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token r5liuw.m9dedii248cqkq0q \
I0829 08:53:16.277708   43612 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:3f45f9a29c7e438a11a7d15ffb056c1db2c3099fb6053195a5114c88bfcf2e6a \
I0829 08:53:16.277735   43612 kubeadm.go:322] 	--control-plane 
I0829 08:53:16.277740   43612 kubeadm.go:322] 
I0829 08:53:16.277868   43612 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0829 08:53:16.277873   43612 kubeadm.go:322] 
I0829 08:53:16.277982   43612 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token r5liuw.m9dedii248cqkq0q \
I0829 08:53:16.278108   43612 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:3f45f9a29c7e438a11a7d15ffb056c1db2c3099fb6053195a5114c88bfcf2e6a 
I0829 08:53:16.284490   43612 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0829 08:53:16.284510   43612 cni.go:84] Creating CNI manager for ""
I0829 08:53:16.284530   43612 cni.go:158] "podman" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0829 08:53:16.290352   43612 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0829 08:53:16.300474   43612 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0829 08:53:16.356394   43612 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0829 08:53:16.448836   43612 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0829 08:53:16.449772   43612 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.4/kubectl label nodes minikube.k8s.io/version=v1.31.2 minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_08_29T08_53_16_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0829 08:53:16.449798   43612 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.4/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0829 08:53:16.507507   43612 ops.go:34] apiserver oom_adj: -16
I0829 08:53:17.437393   43612 kubeadm.go:1081] duration metric: took 987.928087ms to wait for elevateKubeSystemPrivileges.
I0829 08:53:17.460512   43612 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.27.4/kubectl label nodes minikube.k8s.io/version=v1.31.2 minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_08_29T08_53_16_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig: (1.010688627s)
I0829 08:53:17.461009   43612 kubeadm.go:406] StartCluster complete in 18.057346024s
I0829 08:53:17.461031   43612 settings.go:142] acquiring lock: {Name:mkd85e3c5412832a80fadf310095f7460a783936 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:53:17.462310   43612 settings.go:150] Updating kubeconfig:  /Users/afridermawanginting/.kube/config
I0829 08:53:17.473317   43612 lock.go:35] WriteFile acquiring /Users/afridermawanginting/.kube/config: {Name:mk1409e4e9970025d838e2491a5472e54ab6e6dd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0829 08:53:17.476828   43612 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0829 08:53:17.477213   43612 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0829 08:53:17.477541   43612 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0829 08:53:17.478725   43612 host.go:66] Checking if "minikube" exists ...
I0829 08:53:17.480408   43612 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0829 08:53:17.490873   43612 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0829 08:53:17.491092   43612 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0829 08:53:17.493193   43612 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0829 08:53:17.507448   43612 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0829 08:53:17.514615   43612 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0829 08:53:17.609449   43612 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0829 08:53:17.609483   43612 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0829 08:53:17.618041   43612 out.go:177] 🔎  Verifying Kubernetes components...
I0829 08:53:17.625184   43612 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0829 08:53:18.164043   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:53:18.164100   43612 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           <nil> host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0829 08:53:19.217984   43612 cli_runner.go:217] Completed: podman container inspect minikube --format={{.State.Status}}: (1.724694464s)
I0829 08:53:19.226414   43612 cli_runner.go:217] Completed: podman container inspect minikube --format={{.State.Status}}: (1.745911002s)
I0829 08:53:19.230975   43612 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0829 08:53:19.234430   43612 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0829 08:53:19.234441   43612 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0829 08:53:19.234674   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:53:19.329497   43612 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0829 08:53:19.329543   43612 host.go:66] Checking if "minikube" exists ...
I0829 08:53:19.330293   43612 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I0829 08:53:20.070854   43612 cli_runner.go:217] Completed: podman version --format {{.Version}}: (1.906704806s)
I0829 08:53:20.071162   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0829 08:53:20.204917   43612 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0829 08:53:20.204944   43612 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0829 08:53:20.205179   43612 cli_runner.go:164] Run: podman version --format {{.Version}}
I0829 08:53:20.370114   43612 cli_runner.go:217] Completed: podman version --format {{.Version}}: (1.13533241s)
I0829 08:53:20.370427   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:53:20.512680   43612 api_server.go:52] waiting for apiserver process to appear ...
I0829 08:53:20.513115   43612 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0829 08:53:20.761251   43612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:33227 SSHKeyPath:/Users/afridermawanginting/.minikube/machines/minikube/id_rsa Username:docker}
I0829 08:53:20.922033   43612 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0829 08:53:21.038529   43612 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0829 08:53:21.139906   43612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:33227 SSHKeyPath:/Users/afridermawanginting/.minikube/machines/minikube/id_rsa Username:docker}
I0829 08:53:21.289440   43612 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0829 08:53:21.342140   43612 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           <nil> host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (3.177923167s)
I0829 08:53:21.342155   43612 start.go:901] {"host.minikube.internal": <nil>} host record injected into CoreDNS's ConfigMap
I0829 08:53:21.342164   43612 api_server.go:72] duration metric: took 3.732539667s to wait for apiserver process to appear ...
I0829 08:53:21.342172   43612 api_server.go:88] waiting for apiserver healthz status ...
I0829 08:53:21.342221   43612 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:40131/healthz ...
I0829 08:53:21.353423   43612 api_server.go:279] https://127.0.0.1:40131/healthz returned 200:
ok
I0829 08:53:21.355922   43612 api_server.go:141] control plane version: v1.27.4
I0829 08:53:21.355939   43612 api_server.go:131] duration metric: took 13.761265ms to wait for apiserver health ...
I0829 08:53:21.356303   43612 system_pods.go:43] waiting for kube-system pods to appear ...
I0829 08:53:21.372363   43612 system_pods.go:59] 4 kube-system pods found
I0829 08:53:21.372396   43612 system_pods.go:61] "etcd-minikube" [e4c50fc4-ec1d-4e85-89b7-37bdb85597d8] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0829 08:53:21.372413   43612 system_pods.go:61] "kube-apiserver-minikube" [845b63b0-b75d-46fc-9b0d-597cc1693491] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0829 08:53:21.372423   43612 system_pods.go:61] "kube-controller-manager-minikube" [ec0faead-2cf7-4e94-a489-0da6242c9474] Running
I0829 08:53:21.372429   43612 system_pods.go:61] "kube-scheduler-minikube" [47bea878-ce63-4f28-8319-7eac991089ab] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0829 08:53:21.372438   43612 system_pods.go:74] duration metric: took 16.125448ms to wait for pod list to return data ...
I0829 08:53:21.372450   43612 kubeadm.go:581] duration metric: took 3.762826872s to wait for : map[apiserver:true system_pods:true] ...
I0829 08:53:21.372464   43612 node_conditions.go:102] verifying NodePressure condition ...
I0829 08:53:21.377458   43612 node_conditions.go:122] node storage ephemeral capacity is 104266732Ki
I0829 08:53:21.377474   43612 node_conditions.go:123] node cpu capacity is 2
I0829 08:53:21.377787   43612 node_conditions.go:105] duration metric: took 5.312627ms to run NodePressure ...
I0829 08:53:21.377800   43612 start.go:228] waiting for startup goroutines ...
I0829 08:53:22.247580   43612 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.208979407s)
I0829 08:53:22.262989   43612 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0829 08:53:22.267534   43612 addons.go:502] enable addons completed in 4.791448711s: enabled=[storage-provisioner default-storageclass]
I0829 08:53:22.267585   43612 start.go:233] waiting for cluster config update ...
I0829 08:53:22.267609   43612 start.go:242] writing updated cluster config ...
I0829 08:53:22.272781   43612 ssh_runner.go:195] Run: rm -f paused
I0829 08:53:22.886112   43612 start.go:600] kubectl: 1.27.3, cluster: 1.27.4 (minor skew: 0)
I0829 08:53:22.891381   43612 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Aug 29 02:59:18 minikube dockerd[964]: time="2023-08-29T02:59:18.068519602Z" level=info msg="ignoring event" container=542c86cb42843d518f1caf6f0f3495a7c5bb242a56e3713fdfb77d2c8dee4a3e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 02:59:18 minikube dockerd[964]: time="2023-08-29T02:59:18.076198458Z" level=info msg="ignoring event" container=e3267e7d718c11ead2f6340989689e6ac55495f2ad51632e0a1e72ba0bfc2509 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 02:59:18 minikube dockerd[964]: time="2023-08-29T02:59:18.475280735Z" level=info msg="ignoring event" container=d398bd373324ca4a4e3f5e5e5030664fe824bcd05ae99d8cbfb445c6f98c183e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 02:59:18 minikube dockerd[964]: time="2023-08-29T02:59:18.484477607Z" level=info msg="ignoring event" container=f3b674026147f2e9f46057bc2b05d625e0d33c196b1c6302dcf2bb55bf85763b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 02:59:18 minikube cri-dockerd[1176]: time="2023-08-29T02:59:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"karsajobs-6f944db4-4b7s8_default\": unexpected command output nsenter: cannot open /proc/32706/ns/net: No such file or directory\n with error: exit status 1"
Aug 29 03:00:30 minikube dockerd[964]: time="2023-08-29T03:00:30.213670026Z" level=info msg="ignoring event" container=499e0407e793f9460dcd1e65e2852ccf965d97545ccc38c5d5b6e95271631178 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:00:30 minikube dockerd[964]: time="2023-08-29T03:00:30.479903521Z" level=info msg="ignoring event" container=592eec5d2f05a5bb1f6e10eeadd794172c135f811d68b46720f1fc53b268de1f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:00:32 minikube cri-dockerd[1176]: time="2023-08-29T03:00:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/40ce882b55f33e8a1862cde15fbc6f7dc973aa54cb86d8363eda7a3d8da9a2d1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:00:35 minikube cri-dockerd[1176]: time="2023-08-29T03:00:35Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:00:38 minikube dockerd[964]: time="2023-08-29T03:00:38.198322343Z" level=info msg="ignoring event" container=53f0a7b5165f75e58842cc4e69828b2e6509dfac0ed1b3eafbf1581bb8dac37b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:00:38 minikube dockerd[964]: time="2023-08-29T03:00:38.537214195Z" level=info msg="ignoring event" container=84bddcecb47469e3cd99f8deeeaaeeb5261ea0d37f04bfe2031576e93a5011c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:00:41 minikube cri-dockerd[1176]: time="2023-08-29T03:00:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/594a7db994f56bc6f8ea75d8c8df2cc6b039b6c5081c5ad11983d9a8ed7a7199/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:00:42 minikube dockerd[964]: time="2023-08-29T03:00:42.938530706Z" level=info msg="ignoring event" container=e89ee39d8fc4d5ea89aa2a1a2ef3ff6ec05a0aa918c64cd2a8b758cc063ced88 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:00:43 minikube dockerd[964]: time="2023-08-29T03:00:43.197526661Z" level=info msg="ignoring event" container=8d94d8e27e480ca76d0e60e33926b87ec7bc0d90ac0673cf071f69fa71b509de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:00:44 minikube cri-dockerd[1176]: time="2023-08-29T03:00:44Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:00:58 minikube cri-dockerd[1176]: time="2023-08-29T03:00:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8ee8516c96d586fa984771b22ff7592d598d9e9c67b4c15081495dfbc4d92054/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:01:01 minikube cri-dockerd[1176]: time="2023-08-29T03:01:01Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:11:05 minikube dockerd[964]: time="2023-08-29T03:11:05.113135644Z" level=info msg="ignoring event" container=52b00f631dc4f1148542ab66512f47508a61daffe4dc3211b6fb18e2f5cf831c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:11:05 minikube dockerd[964]: time="2023-08-29T03:11:05.339954382Z" level=info msg="ignoring event" container=8ee8516c96d586fa984771b22ff7592d598d9e9c67b4c15081495dfbc4d92054 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:11:06 minikube cri-dockerd[1176]: time="2023-08-29T03:11:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3ae6a52098285d4f8a6e1511eb59b5d29d2367a985a9da200d9ead2a40c82f01/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:11:09 minikube cri-dockerd[1176]: time="2023-08-29T03:11:09Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:11:17 minikube dockerd[964]: time="2023-08-29T03:11:17.154640546Z" level=info msg="ignoring event" container=908e4687cc28744fee96f8bc6e560cff87f6f8e99510aa5b3127461a62859748 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:11:17 minikube dockerd[964]: time="2023-08-29T03:11:17.322525901Z" level=info msg="ignoring event" container=594a7db994f56bc6f8ea75d8c8df2cc6b039b6c5081c5ad11983d9a8ed7a7199 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:11:19 minikube cri-dockerd[1176]: time="2023-08-29T03:11:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3b53a9760b9b35ec553bcb278d69c91eb579459dbf49f5de571ce7f799d50512/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:11:22 minikube dockerd[964]: time="2023-08-29T03:11:22.268643223Z" level=info msg="ignoring event" container=aa06ddee7cb4de02cf6fd7e70a0e52bfb1362c438c3c31bb7ef58b4cb44eea19 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:11:22 minikube cri-dockerd[1176]: time="2023-08-29T03:11:22Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:11:22 minikube dockerd[964]: time="2023-08-29T03:11:22.467028845Z" level=info msg="ignoring event" container=40ce882b55f33e8a1862cde15fbc6f7dc973aa54cb86d8363eda7a3d8da9a2d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:11:23 minikube cri-dockerd[1176]: time="2023-08-29T03:11:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ee811c3ab1edb1791799136d3f8a615e4c32528e41a6382b83dce44fafc0f1e0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:11:26 minikube cri-dockerd[1176]: time="2023-08-29T03:11:26Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:56:35 minikube dockerd[964]: time="2023-08-29T03:56:35.945611918Z" level=info msg="ignoring event" container=a60762384d4be3d367876079fe639a381c79ce05318cc1e89c803d08b120cfe6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:56:35 minikube dockerd[964]: time="2023-08-29T03:56:35.969385585Z" level=info msg="ignoring event" container=d6a4cea0521093a239468b70917562cfa5f1eaa03305dabe28e895c82467e2e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:56:35 minikube dockerd[964]: time="2023-08-29T03:56:35.983165409Z" level=info msg="ignoring event" container=c991f4d88bd6e129cc5aa85d99d20e13d169cd8e3e117851b6f174cd4962efbb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:56:36 minikube dockerd[964]: time="2023-08-29T03:56:36.392995941Z" level=info msg="ignoring event" container=3ae6a52098285d4f8a6e1511eb59b5d29d2367a985a9da200d9ead2a40c82f01 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:56:36 minikube dockerd[964]: time="2023-08-29T03:56:36.414486681Z" level=info msg="ignoring event" container=ee811c3ab1edb1791799136d3f8a615e4c32528e41a6382b83dce44fafc0f1e0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:56:36 minikube dockerd[964]: time="2023-08-29T03:56:36.898861800Z" level=info msg="ignoring event" container=3b53a9760b9b35ec553bcb278d69c91eb579459dbf49f5de571ce7f799d50512 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:56:59 minikube cri-dockerd[1176]: time="2023-08-29T03:56:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/55ed250f7fec62ad8a312b016733e4985070b0178e222386e9afef040cbd4529/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:57:02 minikube cri-dockerd[1176]: time="2023-08-29T03:57:02Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:57:05 minikube cri-dockerd[1176]: time="2023-08-29T03:57:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c1b2693aef0da804646bfb05141dea29dad7b21119cb8e9c8202f3d6552523c6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:57:08 minikube cri-dockerd[1176]: time="2023-08-29T03:57:08Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:57:09 minikube dockerd[964]: time="2023-08-29T03:57:09.313120627Z" level=info msg="ignoring event" container=1e19fab71f0ee0fb9386bed317cca6131090588cc018f3a4a9a0fe052a3e4522 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:57:13 minikube cri-dockerd[1176]: time="2023-08-29T03:57:13Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:57:13 minikube dockerd[964]: time="2023-08-29T03:57:13.352046598Z" level=info msg="ignoring event" container=2f50ff099a02e73ae77419dd03963c310227173c6f8be988689646dbff40e1e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:57:28 minikube cri-dockerd[1176]: time="2023-08-29T03:57:28Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:57:29 minikube dockerd[964]: time="2023-08-29T03:57:29.028358355Z" level=info msg="ignoring event" container=37fcd13cbd3d488f6a3f58126b8954bc6ff5f50bc5c318661dac54641cebf3fb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:57:58 minikube dockerd[964]: time="2023-08-29T03:57:58.514722102Z" level=info msg="ignoring event" container=c1b2693aef0da804646bfb05141dea29dad7b21119cb8e9c8202f3d6552523c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:57:58 minikube dockerd[964]: time="2023-08-29T03:57:58.611852775Z" level=info msg="ignoring event" container=ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:57:58 minikube dockerd[964]: time="2023-08-29T03:57:58.728734113Z" level=info msg="ignoring event" container=55ed250f7fec62ad8a312b016733e4985070b0178e222386e9afef040cbd4529 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 29 03:58:43 minikube cri-dockerd[1176]: time="2023-08-29T03:58:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a4fd4e55679e07c2b278769a5f0b5c21dabe241546d43dd468ee2c7dc2d1af9d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:58:46 minikube cri-dockerd[1176]: time="2023-08-29T03:58:46Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Aug 29 03:59:32 minikube cri-dockerd[1176]: time="2023-08-29T03:59:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cd3013301a670dc95dc97811f7fcea0234b9a58ae5fec1586bde497ba9ebfe82/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 03:59:35 minikube cri-dockerd[1176]: time="2023-08-29T03:59:35Z" level=info msg="Stop pulling image afrimuhsin/karsajobs:latest: Status: Image is up to date for afrimuhsin/karsajobs:latest"
Aug 29 04:31:45 minikube cri-dockerd[1176]: time="2023-08-29T04:31:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1ccd943523f94537a5407b534da375143806f471976486988fb451509c4b8db/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local dns.podman options ndots:5]"
Aug 29 04:32:00 minikube cri-dockerd[1176]: time="2023-08-29T04:32:00Z" level=info msg="Pulling image afrimuhsin/karsajobs-ui:latest: 8f665685b215: Downloading [===============================>                   ]  23.42MB/37.17MB"
Aug 29 04:32:09 minikube cri-dockerd[1176]: time="2023-08-29T04:32:09Z" level=info msg="Pulling image afrimuhsin/karsajobs-ui:latest: 8f665685b215: Extracting [===================================>               ]  26.35MB/37.17MB"
Aug 29 04:32:19 minikube cri-dockerd[1176]: time="2023-08-29T04:32:19Z" level=info msg="Pulling image afrimuhsin/karsajobs-ui:latest: b0f57b992792: Downloading [==============================================>    ]  62.49MB/67.72MB"
Aug 29 04:32:29 minikube cri-dockerd[1176]: time="2023-08-29T04:32:29Z" level=info msg="Pulling image afrimuhsin/karsajobs-ui:latest: b0f57b992792: Extracting [====>                                              ]  5.571MB/67.72MB"
Aug 29 04:32:39 minikube cri-dockerd[1176]: time="2023-08-29T04:32:39Z" level=info msg="Pulling image afrimuhsin/karsajobs-ui:latest: b0f57b992792: Extracting [==========>                                        ]  14.48MB/67.72MB"
Aug 29 04:32:49 minikube cri-dockerd[1176]: time="2023-08-29T04:32:49Z" level=info msg="Pulling image afrimuhsin/karsajobs-ui:latest: b0f57b992792: Extracting [=================>                                 ]   23.4MB/67.72MB"
Aug 29 04:32:59 minikube cri-dockerd[1176]: time="2023-08-29T04:32:59Z" level=info msg="Pulling image afrimuhsin/karsajobs-ui:latest: b0f57b992792: Extracting [=============================>                     ]  40.11MB/67.72MB"
Aug 29 04:33:06 minikube cri-dockerd[1176]: time="2023-08-29T04:33:06Z" level=info msg="Stop pulling image afrimuhsin/karsajobs-ui:latest: Status: Downloaded newer image for afrimuhsin/karsajobs-ui:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
57084d9a73b78       afrimuhsin/karsajobs-ui@sha256:6f3258ab354932d4f1c7c30c2ccc126395e14a81044f8753489fd8b58174b1ed                         5 minutes ago       Running             karsajobs-ui                0                   d1ccd943523f9       karsajobs-ui-54bc8cb55b-gflk8
372464e1a503f       afrimuhsin/karsajobs@sha256:877b03ff48f69207f3cf937c4a4d7353483192c2545202981477c899a4f47124                            39 minutes ago      Running             karsajobs                   0                   cd3013301a670       karsajobs-c79c8864b-kbp7n
3190b860a1552       mongo@sha256:a89d79ddc5187f57b1270f87ec581b7cc6fd697efa12b8f1af72f3c4888d72b5                                           39 minutes ago      Running             mongodb                     0                   a4fd4e55679e0       mongodb-statefulset-0
57451ecc0eff0       registry.k8s.io/metrics-server/metrics-server@sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e   3 hours ago         Running             metrics-server              0                   1ac1d58aaf573       metrics-server-7746886d4f-g85rn
e6f91830e1a60       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93                          3 hours ago         Running             kubernetes-dashboard        0                   8182720be9a93       kubernetes-dashboard-5c5cfc8747-qrg2b
69ee77b4b7c67       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c                    3 hours ago         Running             dashboard-metrics-scraper   0                   2ce088db5debc       dashboard-metrics-scraper-5dd9cbfd69-6p8fx
18433731cced9       6e38f40d628db                                                                                                           3 hours ago         Running             storage-provisioner         1                   6e7932bbc88b7       storage-provisioner
4eeba73cdbfba       6848d7eda0341                                                                                                           3 hours ago         Running             kube-proxy                  0                   c0903ec2939c6       kube-proxy-78l42
f1c9c9f2a86b1       ead0a4a53df89                                                                                                           3 hours ago         Running             coredns                     0                   07a74360cc1ac       coredns-5d78c9869d-sdlvc
0e064a9d595fe       6e38f40d628db                                                                                                           3 hours ago         Exited              storage-provisioner         0                   6e7932bbc88b7       storage-provisioner
c14589e8c9a6b       98ef2570f3cde                                                                                                           3 hours ago         Running             kube-scheduler              0                   ab9da6410e7fe       kube-scheduler-minikube
a1f04fa0bb2d0       86b6af7dd652c                                                                                                           3 hours ago         Running             etcd                        0                   8b4ebfbf08b5b       etcd-minikube
2cb6911f430aa       f466468864b7a                                                                                                           3 hours ago         Running             kube-controller-manager     0                   aa808a01d6ad0       kube-controller-manager-minikube
d9daf03b7c33f       e7972205b6614                                                                                                           3 hours ago         Running             kube-apiserver              0                   a5eb09ab5b940       kube-apiserver-minikube

* 
* ==> coredns [f1c9c9f2a86b] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = b69cf0c8e938111fb046045ef5ed9a23341ad3db13dfbef74baee330cc03dc3bfdcdd95cbb2df69ce4eed7a0c8a70ef9ed4e0226368346980b3b0be75428cc6d
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:56446 - 27692 "HINFO IN 3738226433583756248.5740198477558518479. udp 57 false 512" NOERROR qr,rd,ra 57 0.017116528s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.19:39975 - 11628 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00157472s
[INFO] 10.244.0.19:49349 - 21068 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.067678021s
[INFO] 10.244.0.19:46146 - 26937 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.003142162s
[INFO] 10.244.0.19:38227 - 45470 "A IN compass.mongodb.com.dns.podman. udp 48 false 512" NXDOMAIN qr,rd,ra 48 0.051446136s
[INFO] 10.244.0.19:44828 - 10963 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 212 0.046806846s
[INFO] 10.244.0.19:35259 - 49075 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000203523s
[INFO] 10.244.0.19:46188 - 29962 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.00015302s
[INFO] 10.244.0.19:56994 - 19536 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.00017809s
[INFO] 10.244.0.19:48351 - 47149 "A IN api.segment.io.dns.podman. udp 43 false 512" NXDOMAIN qr,rd,ra 43 0.000302755s
[INFO] 10.244.0.19:58964 - 30286 "A IN api.segment.io. udp 32 false 512" NOERROR qr,rd,ra 122 0.024432541s
[INFO] 10.244.0.19:56461 - 42191 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000165872s
[INFO] 10.244.0.19:42964 - 34225 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000052824s
[INFO] 10.244.0.19:39894 - 36836 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000087014s
[INFO] 10.244.0.19:38496 - 211 "A IN api.segment.io.dns.podman. udp 43 false 512" NXDOMAIN qr,rd,ra 43 0.002436692s
[INFO] 10.244.0.19:54485 - 24454 "A IN api.segment.io. udp 32 false 512" NOERROR qr,aa,rd,ra 122 0.000208282s
[INFO] 10.244.0.19:42941 - 368 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000154776s
[INFO] 10.244.0.19:32894 - 45528 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000223604s
[INFO] 10.244.0.19:35017 - 3689 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000110973s
[INFO] 10.244.0.19:56214 - 42016 "A IN raw.githubusercontent.com.dns.podman. udp 54 false 512" NXDOMAIN qr,rd,ra 54 0.000655267s
[INFO] 10.244.0.19:47827 - 57166 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd,ra 207 0.028564171s
[INFO] 10.244.0.22:41462 - 46739 "AAAA IN mongo-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.002473429s
[INFO] 10.244.0.22:37239 - 11515 "A IN mongo-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.007332033s
[INFO] 10.244.0.22:37754 - 45259 "A IN mongo-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000556769s
[INFO] 10.244.0.22:48372 - 31521 "AAAA IN mongo-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.00026638s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_29T08_53_16_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 29 Aug 2023 01:53:13 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 29 Aug 2023 04:38:39 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 29 Aug 2023 04:38:37 +0000   Tue, 29 Aug 2023 01:53:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 29 Aug 2023 04:38:37 +0000   Tue, 29 Aug 2023 01:53:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 29 Aug 2023 04:38:37 +0000   Tue, 29 Aug 2023 01:53:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 29 Aug 2023 04:38:37 +0000   Tue, 29 Aug 2023 01:53:27 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  104266732Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2000436Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  104266732Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2000436Ki
  pods:               110
System Info:
  Machine ID:                 ff01d8e7b85a42a2892f783e31008630
  System UUID:                ff01d8e7b85a42a2892f783e31008630
  Boot ID:                    9305ef8c-8d21-4dd8-8265-7cc8183eb494
  Kernel Version:             6.4.11-200.fc38.x86_64
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     karsajobs-c79c8864b-kbp7n                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         39m
  default                     karsajobs-ui-54bc8cb55b-gflk8                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m59s
  default                     mongodb-statefulset-0                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         40m
  kube-system                 coredns-5d78c9869d-sdlvc                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (3%!)(MISSING)        170Mi (8%!)(MISSING)     165m
  kube-system                 etcd-minikube                                 100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (5%!)(MISSING)       0 (0%!)(MISSING)         165m
  kube-system                 kube-apiserver-minikube                       250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         165m
  kube-system                 kube-controller-manager-minikube              200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         165m
  kube-system                 kube-proxy-78l42                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         165m
  kube-system                 kube-scheduler-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         165m
  kube-system                 metrics-server-7746886d4f-g85rn               100m (5%!)(MISSING)     0 (0%!)(MISSING)      200Mi (10%!)(MISSING)      0 (0%!)(MISSING)         155m
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         165m
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-6p8fx    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         157m
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-qrg2b         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         157m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                850m (42%!)(MISSING)   0 (0%!)(MISSING)
  memory             370Mi (18%!)(MISSING)  170Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +5.007898] kauditd_printk_skb: 225 callbacks suppressed
[  +5.167621] kauditd_printk_skb: 48 callbacks suppressed
[ +32.654664] kauditd_printk_skb: 225 callbacks suppressed
[Aug29 01:54] kauditd_printk_skb: 4 callbacks suppressed
[ +45.946715] kauditd_printk_skb: 149 callbacks suppressed
[Aug29 01:56] kauditd_printk_skb: 51 callbacks suppressed
[Aug29 02:00] kauditd_printk_skb: 312 callbacks suppressed
[Aug29 02:01] kauditd_printk_skb: 38 callbacks suppressed
[ +27.372734] kauditd_printk_skb: 7 callbacks suppressed
[Aug29 02:03] kauditd_printk_skb: 7 callbacks suppressed
[  +5.990561] kauditd_printk_skb: 163 callbacks suppressed
[ +16.858998] kauditd_printk_skb: 31 callbacks suppressed
[Aug29 02:04] kauditd_printk_skb: 7 callbacks suppressed
[Aug29 02:05] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 02:07] kauditd_printk_skb: 16 callbacks suppressed
[  +5.802450] kauditd_printk_skb: 33 callbacks suppressed
[  +7.153897] kauditd_printk_skb: 33 callbacks suppressed
[Aug29 02:16] kauditd_printk_skb: 7 callbacks suppressed
[ +12.517508] kauditd_printk_skb: 3 callbacks suppressed
[Aug29 02:18] kauditd_printk_skb: 42 callbacks suppressed
[Aug29 02:20] kauditd_printk_skb: 24 callbacks suppressed
[Aug29 02:21] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 02:29] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[Aug29 02:30] kauditd_printk_skb: 18 callbacks suppressed
[  +5.396184] kauditd_printk_skb: 2 callbacks suppressed
[  +7.016966] kauditd_printk_skb: 59 callbacks suppressed
[Aug29 02:31] kauditd_printk_skb: 7 callbacks suppressed
[  +5.024807] kauditd_printk_skb: 195 callbacks suppressed
[  +5.543947] kauditd_printk_skb: 18 callbacks suppressed
[Aug29 02:58] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 02:59] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 03:00] kauditd_printk_skb: 18 callbacks suppressed
[  +5.142816] kauditd_printk_skb: 30 callbacks suppressed
[  +5.439330] kauditd_printk_skb: 15 callbacks suppressed
[ +17.400670] kauditd_printk_skb: 41 callbacks suppressed
[Aug29 03:11] kauditd_printk_skb: 27 callbacks suppressed
[ +12.031387] kauditd_printk_skb: 41 callbacks suppressed
[  +5.113397] kauditd_printk_skb: 30 callbacks suppressed
[Aug29 03:16] kauditd_printk_skb: 52 callbacks suppressed
[Aug29 03:46] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 03:56] kauditd_printk_skb: 56 callbacks suppressed
[Aug29 03:57] kauditd_printk_skb: 33 callbacks suppressed
[  +7.591717] kauditd_printk_skb: 34 callbacks suppressed
[ +15.621388] kauditd_printk_skb: 2 callbacks suppressed
[ +29.603519] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 03:58] kauditd_printk_skb: 35 callbacks suppressed
[ +18.910040] kauditd_printk_skb: 2 callbacks suppressed
[  +8.046326] kauditd_printk_skb: 28 callbacks suppressed
[Aug29 03:59] kauditd_printk_skb: 7 callbacks suppressed
[  +8.111706] kauditd_printk_skb: 28 callbacks suppressed
[Aug29 04:16] kauditd_printk_skb: 7 callbacks suppressed
[Aug29 04:25] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 04:31] kauditd_printk_skb: 2 callbacks suppressed
[Aug29 04:33] kauditd_printk_skb: 28 callbacks suppressed
[Aug29 04:34] kauditd_printk_skb: 7 callbacks suppressed
[  +5.305093] kauditd_printk_skb: 26 callbacks suppressed
[  +7.118123] kauditd_printk_skb: 147 callbacks suppressed
[Aug29 04:35] kauditd_printk_skb: 183 callbacks suppressed
[Aug29 04:38] kauditd_printk_skb: 149 callbacks suppressed
[ +18.303557] kauditd_printk_skb: 121 callbacks suppressed

* 
* ==> etcd [a1f04fa0bb2d] <==
* {"level":"info","ts":"2023-08-29T03:53:11.629Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":429127778,"revision":6359,"compact-revision":6118}
{"level":"info","ts":"2023-08-29T03:53:37.718Z","caller":"traceutil/trace.go:171","msg":"trace[1271028859] transaction","detail":"{read_only:false; response_revision:6620; number_of_response:1; }","duration":"109.189586ms","start":"2023-08-29T03:53:37.607Z","end":"2023-08-29T03:53:37.716Z","steps":["trace[1271028859] 'process raft request'  (duration: 104.573643ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-29T03:56:38.082Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.424274ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-29T03:56:38.102Z","caller":"traceutil/trace.go:171","msg":"trace[1873008207] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6784; }","duration":"145.155945ms","start":"2023-08-29T03:56:37.954Z","end":"2023-08-29T03:56:38.099Z","steps":["trace[1873008207] 'agreement among raft nodes before linearized reading'  (duration: 123.051598ms)"],"step_count":1}
{"level":"info","ts":"2023-08-29T03:56:38.104Z","caller":"traceutil/trace.go:171","msg":"trace[287739725] linearizableReadLoop","detail":"{readStateIndex:8310; appliedIndex:8310; }","duration":"117.224985ms","start":"2023-08-29T03:56:37.986Z","end":"2023-08-29T03:56:38.103Z","steps":["trace[287739725] 'read index received'  (duration: 117.221047ms)","trace[287739725] 'applied index is now lower than readState.Index'  (duration: 3.135µs)"],"step_count":2}
{"level":"warn","ts":"2023-08-29T03:56:38.105Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"118.323226ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-29T03:56:38.105Z","caller":"traceutil/trace.go:171","msg":"trace[1483512212] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:6784; }","duration":"119.187313ms","start":"2023-08-29T03:56:37.986Z","end":"2023-08-29T03:56:38.105Z","steps":["trace[1483512212] 'agreement among raft nodes before linearized reading'  (duration: 118.246132ms)"],"step_count":1}
{"level":"info","ts":"2023-08-29T03:56:38.302Z","caller":"traceutil/trace.go:171","msg":"trace[935163355] transaction","detail":"{read_only:false; response_revision:6786; number_of_response:1; }","duration":"104.233304ms","start":"2023-08-29T03:56:38.198Z","end":"2023-08-29T03:56:38.302Z","steps":["trace[935163355] 'process raft request'  (duration: 22.275016ms)","trace[935163355] 'compare'  (duration: 80.225058ms)"],"step_count":2}
{"level":"info","ts":"2023-08-29T03:58:11.641Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6598}
{"level":"info","ts":"2023-08-29T03:58:11.643Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6598,"took":"1.281434ms","hash":753291035}
{"level":"info","ts":"2023-08-29T03:58:11.643Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":753291035,"revision":6598,"compact-revision":6359}
{"level":"info","ts":"2023-08-29T04:03:11.654Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6967}
{"level":"info","ts":"2023-08-29T04:03:11.667Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6967,"took":"9.514395ms","hash":423707003}
{"level":"info","ts":"2023-08-29T04:03:11.667Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":423707003,"revision":6967,"compact-revision":6598}
{"level":"warn","ts":"2023-08-29T04:05:38.880Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"247.047693ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2023-08-29T04:05:38.891Z","caller":"traceutil/trace.go:171","msg":"trace[390199625] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:7374; }","duration":"257.683134ms","start":"2023-08-29T04:05:38.629Z","end":"2023-08-29T04:05:38.887Z","steps":["trace[390199625] 'agreement among raft nodes before linearized reading'  (duration: 108.543383ms)","trace[390199625] 'range keys from in-memory index tree'  (duration: 138.432838ms)"],"step_count":2}
{"level":"info","ts":"2023-08-29T04:05:38.912Z","caller":"traceutil/trace.go:171","msg":"trace[1438242683] linearizableReadLoop","detail":"{readStateIndex:9014; appliedIndex:9013; }","duration":"105.981421ms","start":"2023-08-29T04:05:38.629Z","end":"2023-08-29T04:05:38.735Z","steps":["trace[1438242683] 'read index received'  (duration: 105.734872ms)","trace[1438242683] 'applied index is now lower than readState.Index'  (duration: 245.9µs)"],"step_count":2}
{"level":"warn","ts":"2023-08-29T04:05:38.912Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"281.967034ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-29T04:05:38.912Z","caller":"traceutil/trace.go:171","msg":"trace[185811414] range","detail":"{range_begin:/registry/volumeattachments/; range_end:/registry/volumeattachments0; response_count:0; response_revision:7374; }","duration":"282.051232ms","start":"2023-08-29T04:05:38.630Z","end":"2023-08-29T04:05:38.912Z","steps":["trace[185811414] 'agreement among raft nodes before linearized reading'  (duration: 281.916394ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-29T04:05:38.913Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"270.500573ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-08-29T04:05:38.913Z","caller":"traceutil/trace.go:171","msg":"trace[478823994] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:7374; }","duration":"270.732485ms","start":"2023-08-29T04:05:38.642Z","end":"2023-08-29T04:05:38.913Z","steps":["trace[478823994] 'agreement among raft nodes before linearized reading'  (duration: 270.431135ms)"],"step_count":1}
{"level":"info","ts":"2023-08-29T04:05:38.917Z","caller":"traceutil/trace.go:171","msg":"trace[502596198] transaction","detail":"{read_only:false; response_revision:7374; number_of_response:1; }","duration":"466.978895ms","start":"2023-08-29T04:05:38.270Z","end":"2023-08-29T04:05:38.737Z","steps":["trace[502596198] 'process raft request'  (duration: 465.152133ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-29T04:05:39.005Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-29T04:05:38.270Z","time spent":"649.16312ms","remote":"127.0.0.1:56418","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:7365 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128023430661496204 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2023-08-29T04:08:11.673Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7258}
{"level":"info","ts":"2023-08-29T04:08:11.677Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7258,"took":"3.544639ms","hash":2956497929}
{"level":"info","ts":"2023-08-29T04:08:11.678Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2956497929,"revision":7258,"compact-revision":6967}
{"level":"info","ts":"2023-08-29T04:13:11.683Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7496}
{"level":"info","ts":"2023-08-29T04:13:11.686Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7496,"took":"3.232765ms","hash":3663804710}
{"level":"info","ts":"2023-08-29T04:13:11.686Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3663804710,"revision":7496,"compact-revision":7258}
{"level":"info","ts":"2023-08-29T04:18:11.700Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7737}
{"level":"info","ts":"2023-08-29T04:18:11.708Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7737,"took":"4.744911ms","hash":763205025}
{"level":"info","ts":"2023-08-29T04:18:11.708Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":763205025,"revision":7737,"compact-revision":7496}
{"level":"info","ts":"2023-08-29T04:19:39.303Z","caller":"traceutil/trace.go:171","msg":"trace[1992026204] transaction","detail":"{read_only:false; response_revision:8046; number_of_response:1; }","duration":"117.361205ms","start":"2023-08-29T04:19:39.184Z","end":"2023-08-29T04:19:39.302Z","steps":["trace[1992026204] 'process raft request'  (duration: 116.996268ms)"],"step_count":1}
{"level":"info","ts":"2023-08-29T04:22:04.967Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-08-29T04:22:05.080Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2023-08-29T04:22:05.082Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2023-08-29T04:23:11.713Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7976}
{"level":"info","ts":"2023-08-29T04:23:11.736Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7976,"took":"19.389069ms","hash":2426949023}
{"level":"info","ts":"2023-08-29T04:23:11.737Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2426949023,"revision":7976,"compact-revision":7737}
{"level":"info","ts":"2023-08-29T04:27:34.339Z","caller":"traceutil/trace.go:171","msg":"trace[106412627] linearizableReadLoop","detail":"{readStateIndex:10338; appliedIndex:10337; }","duration":"326.427472ms","start":"2023-08-29T04:27:34.011Z","end":"2023-08-29T04:27:34.338Z","steps":["trace[106412627] 'read index received'  (duration: 325.966383ms)","trace[106412627] 'applied index is now lower than readState.Index'  (duration: 460.293µs)"],"step_count":2}
{"level":"warn","ts":"2023-08-29T04:27:34.342Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"328.791025ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-29T04:27:34.342Z","caller":"traceutil/trace.go:171","msg":"trace[75549691] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8431; }","duration":"330.396084ms","start":"2023-08-29T04:27:34.011Z","end":"2023-08-29T04:27:34.342Z","steps":["trace[75549691] 'agreement among raft nodes before linearized reading'  (duration: 328.735714ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-29T04:27:34.342Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-29T04:27:34.011Z","time spent":"330.540103ms","remote":"127.0.0.1:56810","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-08-29T04:28:11.765Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8215}
{"level":"info","ts":"2023-08-29T04:28:11.781Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8215,"took":"14.964296ms","hash":516642705}
{"level":"info","ts":"2023-08-29T04:28:11.781Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":516642705,"revision":8215,"compact-revision":7976}
{"level":"info","ts":"2023-08-29T04:32:13.048Z","caller":"traceutil/trace.go:171","msg":"trace[1220335547] transaction","detail":"{read_only:false; response_revision:8673; number_of_response:1; }","duration":"174.739134ms","start":"2023-08-29T04:32:12.873Z","end":"2023-08-29T04:32:13.048Z","steps":["trace[1220335547] 'process raft request'  (duration: 174.271297ms)"],"step_count":1}
{"level":"info","ts":"2023-08-29T04:32:15.309Z","caller":"traceutil/trace.go:171","msg":"trace[1628794225] transaction","detail":"{read_only:false; response_revision:8675; number_of_response:1; }","duration":"107.905235ms","start":"2023-08-29T04:32:15.195Z","end":"2023-08-29T04:32:15.303Z","steps":["trace[1628794225] 'process raft request'  (duration: 99.998116ms)"],"step_count":1}
{"level":"info","ts":"2023-08-29T04:33:11.778Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8463}
{"level":"info","ts":"2023-08-29T04:33:11.909Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8463,"took":"128.971081ms","hash":3692812892}
{"level":"info","ts":"2023-08-29T04:33:11.909Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3692812892,"revision":8463,"compact-revision":8215}
{"level":"warn","ts":"2023-08-29T04:34:03.958Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.732992ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-29T04:34:03.962Z","caller":"traceutil/trace.go:171","msg":"trace[142723803] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8766; }","duration":"106.430583ms","start":"2023-08-29T04:34:03.855Z","end":"2023-08-29T04:34:03.962Z","steps":["trace[142723803] 'get authentication metadata'  (duration: 102.502443ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-29T04:34:56.030Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.083782ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-29T04:34:56.032Z","caller":"traceutil/trace.go:171","msg":"trace[464391677] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8808; }","duration":"147.374783ms","start":"2023-08-29T04:34:55.883Z","end":"2023-08-29T04:34:56.031Z","steps":["trace[464391677] 'range keys from in-memory index tree'  (duration: 145.880313ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-29T04:35:29.231Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"140.289329ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/kubernetes\" ","response":"range_response_count:1 size:704"}
{"level":"info","ts":"2023-08-29T04:35:29.233Z","caller":"traceutil/trace.go:171","msg":"trace[1273660275] range","detail":"{range_begin:/registry/services/specs/default/kubernetes; range_end:; response_count:1; response_revision:8835; }","duration":"141.936606ms","start":"2023-08-29T04:35:29.090Z","end":"2023-08-29T04:35:29.232Z","steps":["trace[1273660275] 'range keys from in-memory index tree'  (duration: 140.100193ms)"],"step_count":1}
{"level":"info","ts":"2023-08-29T04:38:11.792Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8727}
{"level":"info","ts":"2023-08-29T04:38:11.856Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8727,"took":"63.324565ms","hash":165754892}
{"level":"info","ts":"2023-08-29T04:38:11.857Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":165754892,"revision":8727,"compact-revision":8463}

* 
* ==> kernel <==
*  04:38:43 up  2:53,  0 users,  load average: 0.59, 1.60, 1.48
Linux minikube 6.4.11-200.fc38.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Aug 16 17:42:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [d9daf03b7c33] <==
* I0829 03:59:12.940555       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 03:59:27.547438       1 alloc.go:330] "allocated clusterIPs" service="default/karsajobs" clusterIPs=map[IPv4:10.100.54.36]
I0829 04:00:12.926563       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:01:12.927134       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:02:12.927480       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:03:12.929506       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:03:13.895093       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:04:12.924914       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:05:12.926003       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:05:39.009911       1 trace.go:219] Trace[655674909]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (29-Aug-2023 04:05:38.120) (total time: 886ms):
Trace[655674909]: ---"initial value restored" 137ms (04:05:38.257)
Trace[655674909]: ---"Txn call completed" 737ms (04:05:39.006)
Trace[655674909]: [886.701277ms] [886.701277ms] END
I0829 04:06:12.936012       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:07:12.925622       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:08:12.926430       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:08:14.011226       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:09:12.924951       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:10:12.924532       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:11:12.930717       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:12:12.924897       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:13:12.926572       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:13:14.024745       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:14:12.938409       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:15:12.924789       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:16:12.923140       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:17:12.928374       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:18:12.927712       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:18:14.125537       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:19:12.928831       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:20:12.953264       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:21:12.926381       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:22:12.931329       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:23:12.925645       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:23:14.135734       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:24:12.925774       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:25:12.924955       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:25:15.811051       1 alloc.go:330] "allocated clusterIPs" service="default/karsajobs" clusterIPs=map[IPv4:10.100.193.74]
I0829 04:26:12.927411       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:27:12.935963       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:28:12.933445       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:28:14.242892       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:29:12.926536       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:30:12.927621       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:31:12.927109       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:31:49.624902       1 alloc.go:330] "allocated clusterIPs" service="default/karsajobs-ui" clusterIPs=map[IPv4:10.98.173.97]
I0829 04:32:13.170531       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:33:12.928129       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:33:14.299838       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:33:50.922487       1 trace.go:219] Trace[549179435]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e47a7e2e-3451-4444-9504-ec07dd362058,client:::1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (29-Aug-2023 04:33:49.750) (total time: 1164ms):
Trace[549179435]: ---"About to write a response" 1151ms (04:33:50.902)
Trace[549179435]: [1.164775343s] [1.164775343s] END
I0829 04:33:51.479379       1 trace.go:219] Trace[1667973182]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (29-Aug-2023 04:33:50.924) (total time: 554ms):
Trace[1667973182]: [554.749682ms] [554.749682ms] END
I0829 04:34:12.927176       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:35:12.926196       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:36:12.925113       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:37:12.929058       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:38:12.926401       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0829 04:38:14.319201       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-controller-manager [2cb6911f430a] <==
* I0829 01:53:28.801640       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-78l42"
I0829 01:53:28.910735       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5d78c9869d-sdlvc"
I0829 02:00:55.705288       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set kubernetes-dashboard-5c5cfc8747 to 1"
I0829 02:00:55.716717       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dashboard-metrics-scraper-5dd9cbfd69 to 1"
I0829 02:00:55.744726       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-5c5cfc8747-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0829 02:00:55.746112       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-5dd9cbfd69-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0829 02:00:55.758394       1 replica_set.go:544] sync "kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" failed with pods "kubernetes-dashboard-5c5cfc8747-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
E0829 02:00:55.784836       1 replica_set.go:544] sync "kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" failed with pods "dashboard-metrics-scraper-5dd9cbfd69-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
E0829 02:00:55.798719       1 replica_set.go:544] sync "kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" failed with pods "kubernetes-dashboard-5c5cfc8747-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
E0829 02:00:55.801220       1 replica_set.go:544] sync "kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" failed with pods "dashboard-metrics-scraper-5dd9cbfd69-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0829 02:00:55.807404       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-5c5cfc8747-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0829 02:00:55.810450       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-5dd9cbfd69-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0829 02:00:55.815473       1 replica_set.go:544] sync "kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" failed with pods "kubernetes-dashboard-5c5cfc8747-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0829 02:00:55.815796       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-5c5cfc8747-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0829 02:00:55.847561       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-5dd9cbfd69-6p8fx"
I0829 02:00:55.871920       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-5c5cfc8747-qrg2b"
I0829 02:03:06.223269       1 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-7746886d4f to 1"
I0829 02:03:06.357360       1 event.go:307] "Event occurred" object="kube-system/metrics-server-7746886d4f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-7746886d4f-g85rn"
E0829 02:03:28.375102       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W0829 02:03:28.735637       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
E0829 02:03:58.394247       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W0829 02:03:58.752808       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
I0829 02:05:23.893793       1 event.go:307] "Event occurred" object="default/mongo-pvc" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0829 02:05:34.637947       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim mongo-data-mongodb-statefulset-0 Pod mongodb-statefulset-0 in StatefulSet mongodb-statefulset success"
I0829 02:05:34.650653       1 event.go:307] "Event occurred" object="default/mongo-data-mongodb-statefulset-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0829 02:05:34.651330       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-0 in StatefulSet mongodb-statefulset successful"
I0829 02:07:01.705883       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim mongo-data-mongodb-statefulset-1 Pod mongodb-statefulset-1 in StatefulSet mongodb-statefulset success"
I0829 02:07:01.719081       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-1 in StatefulSet mongodb-statefulset successful"
I0829 02:07:01.721506       1 event.go:307] "Event occurred" object="default/mongo-data-mongodb-statefulset-1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0829 02:07:08.672658       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim mongo-data-mongodb-statefulset-2 Pod mongodb-statefulset-2 in StatefulSet mongodb-statefulset success"
I0829 02:07:08.708634       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-2 in StatefulSet mongodb-statefulset successful"
I0829 02:07:08.708655       1 event.go:307] "Event occurred" object="default/mongo-data-mongodb-statefulset-2" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0829 02:16:42.563378       1 event.go:307] "Event occurred" object="default/karsajobs" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set karsajobs-764b6558b to 2"
I0829 02:16:42.595779       1 event.go:307] "Event occurred" object="default/karsajobs-764b6558b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-764b6558b-6hcgh"
I0829 02:16:42.618649       1 event.go:307] "Event occurred" object="default/karsajobs-764b6558b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-764b6558b-kbhtw"
I0829 02:30:16.956831       1 event.go:307] "Event occurred" object="default/karsajobs" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set karsajobs-6f944db4 to 2"
I0829 02:30:16.996622       1 event.go:307] "Event occurred" object="default/karsajobs-6f944db4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-6f944db4-s98m7"
I0829 02:30:17.019576       1 event.go:307] "Event occurred" object="default/karsajobs-6f944db4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-6f944db4-4b7s8"
I0829 03:00:31.059315       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-0 in StatefulSet mongodb-statefulset successful"
I0829 03:00:40.123674       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-1 in StatefulSet mongodb-statefulset successful"
I0829 03:00:46.359104       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-2 in StatefulSet mongodb-statefulset successful"
I0829 03:11:06.149706       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-2 in StatefulSet mongodb-statefulset successful"
I0829 03:11:18.663306       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-1 in StatefulSet mongodb-statefulset successful"
I0829 03:11:23.047595       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-0 in StatefulSet mongodb-statefulset successful"
I0829 03:53:25.478161       1 cleaner.go:172] Cleaning CSR "csr-9k7rr" as it is more than 1h0m0s old and approved.
I0829 03:56:35.179928       1 stateful_set.go:458] "StatefulSet has been deleted" key="default/mongodb-statefulset"
W0829 03:56:35.418234       1 endpointslice_controller.go:297] Error syncing endpoint slices for service "default/mongo-service", retrying. Error: EndpointSlice informer cache is out of date
I0829 03:56:56.367517       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim mongo-pvc-mongodb-statefulset-0 Pod mongodb-statefulset-0 in StatefulSet mongodb-statefulset success"
I0829 03:56:56.384864       1 event.go:307] "Event occurred" object="default/mongo-pvc-mongodb-statefulset-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0829 03:56:56.389646       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-0 in StatefulSet mongodb-statefulset successful"
I0829 03:57:03.575126       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim mongo-pvc-mongodb-statefulset-1 Pod mongodb-statefulset-1 in StatefulSet mongodb-statefulset success"
I0829 03:57:03.592366       1 event.go:307] "Event occurred" object="default/mongo-pvc-mongodb-statefulset-1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0829 03:57:03.598910       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-1 in StatefulSet mongodb-statefulset successful"
I0829 03:57:58.263913       1 stateful_set.go:458] "StatefulSet has been deleted" key="default/mongodb-statefulset"
W0829 03:57:58.397450       1 endpointslice_controller.go:297] Error syncing endpoint slices for service "default/mongo-service", retrying. Error: EndpointSlice informer cache is out of date
I0829 03:58:41.763471       1 event.go:307] "Event occurred" object="default/mongodb-statefulset" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-statefulset-0 in StatefulSet mongodb-statefulset successful"
I0829 03:59:31.772893       1 event.go:307] "Event occurred" object="default/karsajobs" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set karsajobs-c79c8864b to 1"
I0829 03:59:31.821381       1 event.go:307] "Event occurred" object="default/karsajobs-c79c8864b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-c79c8864b-kbp7n"
I0829 04:31:44.126758       1 event.go:307] "Event occurred" object="default/karsajobs-ui" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set karsajobs-ui-54bc8cb55b to 1"
I0829 04:31:44.143833       1 event.go:307] "Event occurred" object="default/karsajobs-ui-54bc8cb55b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: karsajobs-ui-54bc8cb55b-gflk8"

* 
* ==> kube-proxy [4eeba73cdbfb] <==
* I0829 01:53:30.478059       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0829 01:53:30.479532       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0829 01:53:30.480019       1 server_others.go:554] "Using iptables proxy"
I0829 01:53:30.643560       1 server_others.go:192] "Using iptables Proxier"
I0829 01:53:30.643954       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0829 01:53:30.644025       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0829 01:53:30.644140       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0829 01:53:30.644242       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0829 01:53:30.649613       1 server.go:658] "Version info" version="v1.27.4"
I0829 01:53:30.650062       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0829 01:53:30.690235       1 config.go:188] "Starting service config controller"
I0829 01:53:30.690485       1 shared_informer.go:311] Waiting for caches to sync for service config
I0829 01:53:30.690592       1 config.go:97] "Starting endpoint slice config controller"
I0829 01:53:30.690673       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0829 01:53:30.798161       1 config.go:315] "Starting node config controller"
I0829 01:53:30.798475       1 shared_informer.go:311] Waiting for caches to sync for node config
I0829 01:53:30.891761       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0829 01:53:30.910047       1 shared_informer.go:318] Caches are synced for node config
I0829 01:53:30.910220       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [c14589e8c9a6] <==
* W0829 01:53:12.988842       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0829 01:53:12.988931       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0829 01:53:12.988956       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0829 01:53:13.114138       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0829 01:53:13.114213       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0829 01:53:13.120083       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0829 01:53:13.120198       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0829 01:53:13.123393       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0829 01:53:13.123645       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0829 01:53:13.161231       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0829 01:53:13.164777       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0829 01:53:13.163402       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0829 01:53:13.168608       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0829 01:53:13.163464       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0829 01:53:13.168873       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0829 01:53:13.163545       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0829 01:53:13.169067       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0829 01:53:13.163704       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0829 01:53:13.169244       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0829 01:53:13.163842       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0829 01:53:13.169288       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0829 01:53:13.163902       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0829 01:53:13.169323       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0829 01:53:13.163993       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0829 01:53:13.169362       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0829 01:53:13.164154       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0829 01:53:13.169401       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0829 01:53:13.164250       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0829 01:53:13.169437       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0829 01:53:13.164335       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0829 01:53:13.169472       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0829 01:53:13.164420       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0829 01:53:13.169514       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0829 01:53:13.164489       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0829 01:53:13.169550       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0829 01:53:13.164642       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0829 01:53:13.169585       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0829 01:53:13.164699       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0829 01:53:13.169617       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0829 01:53:13.998071       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0829 01:53:13.999500       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0829 01:53:14.050465       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0829 01:53:14.050694       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0829 01:53:14.063452       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0829 01:53:14.063548       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0829 01:53:14.073592       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0829 01:53:14.073624       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0829 01:53:14.085268       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0829 01:53:14.085797       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0829 01:53:14.094796       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0829 01:53:14.095471       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0829 01:53:14.146678       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0829 01:53:14.146909       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0829 01:53:14.153056       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0829 01:53:14.153448       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0829 01:53:14.230842       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0829 01:53:14.230934       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0829 01:53:14.392193       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0829 01:53:14.392274       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0829 01:53:16.224105       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Aug 29 03:57:29 minikube kubelet[2185]: E0829 03:57:29.569284    2185 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CrashLoopBackOff: \"back-off 20s restarting failed container=mongodb pod=mongodb-statefulset-1_default(c55d262e-56db-401a-b636-d242a38e7ca4)\"" pod="default/mongodb-statefulset-1" podUID=c55d262e-56db-401a-b636-d242a38e7ca4
Aug 29 03:57:44 minikube kubelet[2185]: I0829 03:57:44.710818    2185 scope.go:115] "RemoveContainer" containerID="37fcd13cbd3d488f6a3f58126b8954bc6ff5f50bc5c318661dac54641cebf3fb"
Aug 29 03:57:44 minikube kubelet[2185]: E0829 03:57:44.711191    2185 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CrashLoopBackOff: \"back-off 20s restarting failed container=mongodb pod=mongodb-statefulset-1_default(c55d262e-56db-401a-b636-d242a38e7ca4)\"" pod="default/mongodb-statefulset-1" podUID=c55d262e-56db-401a-b636-d242a38e7ca4
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.633744    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/c55d262e-56db-401a-b636-d242a38e7ca4-config-volume\") pod \"c55d262e-56db-401a-b636-d242a38e7ca4\" (UID: \"c55d262e-56db-401a-b636-d242a38e7ca4\") "
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.635048    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"mongo-data\" (UniqueName: \"kubernetes.io/host-path/c55d262e-56db-401a-b636-d242a38e7ca4-mongo-pv\") pod \"c55d262e-56db-401a-b636-d242a38e7ca4\" (UID: \"c55d262e-56db-401a-b636-d242a38e7ca4\") "
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.635107    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"credentials-volume\" (UniqueName: \"kubernetes.io/secret/c55d262e-56db-401a-b636-d242a38e7ca4-credentials-volume\") pod \"c55d262e-56db-401a-b636-d242a38e7ca4\" (UID: \"c55d262e-56db-401a-b636-d242a38e7ca4\") "
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.635131    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-z999c\" (UniqueName: \"kubernetes.io/projected/c55d262e-56db-401a-b636-d242a38e7ca4-kube-api-access-z999c\") pod \"c55d262e-56db-401a-b636-d242a38e7ca4\" (UID: \"c55d262e-56db-401a-b636-d242a38e7ca4\") "
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.635204    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/c55d262e-56db-401a-b636-d242a38e7ca4-mongo-pv" (OuterVolumeSpecName: "mongo-data") pod "c55d262e-56db-401a-b636-d242a38e7ca4" (UID: "c55d262e-56db-401a-b636-d242a38e7ca4"). InnerVolumeSpecName "mongo-pv". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: W0829 03:57:58.637354    2185 empty_dir.go:525] Warning: Failed to clear quota on /var/lib/kubelet/pods/c55d262e-56db-401a-b636-d242a38e7ca4/volumes/kubernetes.io~configmap/config-volume: clearQuota called, but quotas disabled
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.637829    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/c55d262e-56db-401a-b636-d242a38e7ca4-config-volume" (OuterVolumeSpecName: "config-volume") pod "c55d262e-56db-401a-b636-d242a38e7ca4" (UID: "c55d262e-56db-401a-b636-d242a38e7ca4"). InnerVolumeSpecName "config-volume". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.644122    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/c55d262e-56db-401a-b636-d242a38e7ca4-credentials-volume" (OuterVolumeSpecName: "credentials-volume") pod "c55d262e-56db-401a-b636-d242a38e7ca4" (UID: "c55d262e-56db-401a-b636-d242a38e7ca4"). InnerVolumeSpecName "credentials-volume". PluginName "kubernetes.io/secret", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.650206    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c55d262e-56db-401a-b636-d242a38e7ca4-kube-api-access-z999c" (OuterVolumeSpecName: "kube-api-access-z999c") pod "c55d262e-56db-401a-b636-d242a38e7ca4" (UID: "c55d262e-56db-401a-b636-d242a38e7ca4"). InnerVolumeSpecName "kube-api-access-z999c". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.735679    2185 reconciler_common.go:300] "Volume detached for volume \"credentials-volume\" (UniqueName: \"kubernetes.io/secret/c55d262e-56db-401a-b636-d242a38e7ca4-credentials-volume\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.736117    2185 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-z999c\" (UniqueName: \"kubernetes.io/projected/c55d262e-56db-401a-b636-d242a38e7ca4-kube-api-access-z999c\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.736189    2185 reconciler_common.go:300] "Volume detached for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/c55d262e-56db-401a-b636-d242a38e7ca4-config-volume\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.736202    2185 reconciler_common.go:300] "Volume detached for volume \"mongo-pv\" (UniqueName: \"kubernetes.io/host-path/c55d262e-56db-401a-b636-d242a38e7ca4-mongo-pv\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.836531    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/50aaf980-0dd5-404d-a397-c5cee43f9016-config-volume\") pod \"50aaf980-0dd5-404d-a397-c5cee43f9016\" (UID: \"50aaf980-0dd5-404d-a397-c5cee43f9016\") "
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.836680    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8f7nv\" (UniqueName: \"kubernetes.io/projected/50aaf980-0dd5-404d-a397-c5cee43f9016-kube-api-access-8f7nv\") pod \"50aaf980-0dd5-404d-a397-c5cee43f9016\" (UID: \"50aaf980-0dd5-404d-a397-c5cee43f9016\") "
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.836723    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"mongo-data\" (UniqueName: \"kubernetes.io/host-path/50aaf980-0dd5-404d-a397-c5cee43f9016-mongo-pv\") pod \"50aaf980-0dd5-404d-a397-c5cee43f9016\" (UID: \"50aaf980-0dd5-404d-a397-c5cee43f9016\") "
Aug 29 03:57:58 minikube kubelet[2185]: W0829 03:57:58.839332    2185 empty_dir.go:525] Warning: Failed to clear quota on /var/lib/kubelet/pods/50aaf980-0dd5-404d-a397-c5cee43f9016/volumes/kubernetes.io~configmap/config-volume: clearQuota called, but quotas disabled
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.841058    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/50aaf980-0dd5-404d-a397-c5cee43f9016-config-volume" (OuterVolumeSpecName: "config-volume") pod "50aaf980-0dd5-404d-a397-c5cee43f9016" (UID: "50aaf980-0dd5-404d-a397-c5cee43f9016"). InnerVolumeSpecName "config-volume". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.842133    2185 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"credentials-volume\" (UniqueName: \"kubernetes.io/secret/50aaf980-0dd5-404d-a397-c5cee43f9016-credentials-volume\") pod \"50aaf980-0dd5-404d-a397-c5cee43f9016\" (UID: \"50aaf980-0dd5-404d-a397-c5cee43f9016\") "
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.842216    2185 reconciler_common.go:300] "Volume detached for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/50aaf980-0dd5-404d-a397-c5cee43f9016-config-volume\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.844990    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/50aaf980-0dd5-404d-a397-c5cee43f9016-mongo-pv" (OuterVolumeSpecName: "mongo-data") pod "50aaf980-0dd5-404d-a397-c5cee43f9016" (UID: "50aaf980-0dd5-404d-a397-c5cee43f9016"). InnerVolumeSpecName "mongo-pv". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.849754    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/50aaf980-0dd5-404d-a397-c5cee43f9016-kube-api-access-8f7nv" (OuterVolumeSpecName: "kube-api-access-8f7nv") pod "50aaf980-0dd5-404d-a397-c5cee43f9016" (UID: "50aaf980-0dd5-404d-a397-c5cee43f9016"). InnerVolumeSpecName "kube-api-access-8f7nv". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.851490    2185 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/50aaf980-0dd5-404d-a397-c5cee43f9016-credentials-volume" (OuterVolumeSpecName: "credentials-volume") pod "50aaf980-0dd5-404d-a397-c5cee43f9016" (UID: "50aaf980-0dd5-404d-a397-c5cee43f9016"). InnerVolumeSpecName "credentials-volume". PluginName "kubernetes.io/secret", VolumeGidValue ""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.943734    2185 reconciler_common.go:300] "Volume detached for volume \"mongo-pv\" (UniqueName: \"kubernetes.io/host-path/50aaf980-0dd5-404d-a397-c5cee43f9016-mongo-pv\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.943863    2185 reconciler_common.go:300] "Volume detached for volume \"credentials-volume\" (UniqueName: \"kubernetes.io/secret/50aaf980-0dd5-404d-a397-c5cee43f9016-credentials-volume\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.943902    2185 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-8f7nv\" (UniqueName: \"kubernetes.io/projected/50aaf980-0dd5-404d-a397-c5cee43f9016-kube-api-access-8f7nv\") on node \"minikube\" DevicePath \"\""
Aug 29 03:57:58 minikube kubelet[2185]: I0829 03:57:58.993560    2185 scope.go:115] "RemoveContainer" containerID="ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad"
Aug 29 03:57:59 minikube kubelet[2185]: I0829 03:57:59.053952    2185 scope.go:115] "RemoveContainer" containerID="ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad"
Aug 29 03:57:59 minikube kubelet[2185]: E0829 03:57:59.056118    2185 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad" containerID="ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad"
Aug 29 03:57:59 minikube kubelet[2185]: I0829 03:57:59.056204    2185 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad} err="failed to get container status \"ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad\": rpc error: code = Unknown desc = Error response from daemon: No such container: ef8701d2173a88fc1efe7b0513eb98d312d53016712e823a7029e820bed2efad"
Aug 29 03:57:59 minikube kubelet[2185]: I0829 03:57:59.056218    2185 scope.go:115] "RemoveContainer" containerID="37fcd13cbd3d488f6a3f58126b8954bc6ff5f50bc5c318661dac54641cebf3fb"
Aug 29 03:58:00 minikube kubelet[2185]: I0829 03:58:00.715645    2185 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=50aaf980-0dd5-404d-a397-c5cee43f9016 path="/var/lib/kubelet/pods/50aaf980-0dd5-404d-a397-c5cee43f9016/volumes"
Aug 29 03:58:00 minikube kubelet[2185]: I0829 03:58:00.719176    2185 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=c55d262e-56db-401a-b636-d242a38e7ca4 path="/var/lib/kubelet/pods/c55d262e-56db-401a-b636-d242a38e7ca4/volumes"
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.792860    2185 topology_manager.go:212] "Topology Admit Handler"
Aug 29 03:58:41 minikube kubelet[2185]: E0829 03:58:41.793086    2185 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="50aaf980-0dd5-404d-a397-c5cee43f9016" containerName="mongodb"
Aug 29 03:58:41 minikube kubelet[2185]: E0829 03:58:41.793111    2185 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c55d262e-56db-401a-b636-d242a38e7ca4" containerName="mongodb"
Aug 29 03:58:41 minikube kubelet[2185]: E0829 03:58:41.793123    2185 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c55d262e-56db-401a-b636-d242a38e7ca4" containerName="mongodb"
Aug 29 03:58:41 minikube kubelet[2185]: E0829 03:58:41.794535    2185 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c55d262e-56db-401a-b636-d242a38e7ca4" containerName="mongodb"
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.794630    2185 memory_manager.go:346] "RemoveStaleState removing state" podUID="c55d262e-56db-401a-b636-d242a38e7ca4" containerName="mongodb"
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.794646    2185 memory_manager.go:346] "RemoveStaleState removing state" podUID="c55d262e-56db-401a-b636-d242a38e7ca4" containerName="mongodb"
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.794657    2185 memory_manager.go:346] "RemoveStaleState removing state" podUID="50aaf980-0dd5-404d-a397-c5cee43f9016" containerName="mongodb"
Aug 29 03:58:41 minikube kubelet[2185]: W0829 03:58:41.807126    2185 reflector.go:533] object-"default"/"mongodb-secret": failed to list *v1.Secret: secrets "mongodb-secret" is forbidden: User "system:node:minikube" cannot list resource "secrets" in API group "" in the namespace "default": no relationship found between node 'minikube' and this object
Aug 29 03:58:41 minikube kubelet[2185]: E0829 03:58:41.810024    2185 reflector.go:148] object-"default"/"mongodb-secret": Failed to watch *v1.Secret: failed to list *v1.Secret: secrets "mongodb-secret" is forbidden: User "system:node:minikube" cannot list resource "secrets" in API group "" in the namespace "default": no relationship found between node 'minikube' and this object
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.876654    2185 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"credentials-volume\" (UniqueName: \"kubernetes.io/secret/06a98e6a-166f-44de-bf0f-6365d3128dd9-credentials-volume\") pod \"mongodb-statefulset-0\" (UID: \"06a98e6a-166f-44de-bf0f-6365d3128dd9\") " pod="default/mongodb-statefulset-0"
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.876735    2185 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"mongo-pv\" (UniqueName: \"kubernetes.io/host-path/06a98e6a-166f-44de-bf0f-6365d3128dd9-mongo-pv\") pod \"mongodb-statefulset-0\" (UID: \"06a98e6a-166f-44de-bf0f-6365d3128dd9\") " pod="default/mongodb-statefulset-0"
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.876847    2185 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8qmhf\" (UniqueName: \"kubernetes.io/projected/06a98e6a-166f-44de-bf0f-6365d3128dd9-kube-api-access-8qmhf\") pod \"mongodb-statefulset-0\" (UID: \"06a98e6a-166f-44de-bf0f-6365d3128dd9\") " pod="default/mongodb-statefulset-0"
Aug 29 03:58:41 minikube kubelet[2185]: I0829 03:58:41.876924    2185 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/06a98e6a-166f-44de-bf0f-6365d3128dd9-config-volume\") pod \"mongodb-statefulset-0\" (UID: \"06a98e6a-166f-44de-bf0f-6365d3128dd9\") " pod="default/mongodb-statefulset-0"
Aug 29 03:59:31 minikube kubelet[2185]: I0829 03:59:31.847312    2185 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/mongodb-statefulset-0" podStartSLOduration=47.932308218 podCreationTimestamp="2023-08-29 03:58:41 +0000 UTC" firstStartedPulling="2023-08-29 03:58:43.400203767 +0000 UTC m=+7527.080168983" lastFinishedPulling="2023-08-29 03:58:46.315107003 +0000 UTC m=+7529.995072229" observedRunningTime="2023-08-29 03:58:46.914382375 +0000 UTC m=+7530.594347604" watchObservedRunningTime="2023-08-29 03:59:31.847211464 +0000 UTC m=+7575.527176692"
Aug 29 03:59:31 minikube kubelet[2185]: I0829 03:59:31.847804    2185 topology_manager.go:212] "Topology Admit Handler"
Aug 29 03:59:31 minikube kubelet[2185]: I0829 03:59:31.847891    2185 memory_manager.go:346] "RemoveStaleState removing state" podUID="c55d262e-56db-401a-b636-d242a38e7ca4" containerName="mongodb"
Aug 29 03:59:31 minikube kubelet[2185]: I0829 03:59:31.926755    2185 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sqt5f\" (UniqueName: \"kubernetes.io/projected/36a0fce7-0d02-41e0-bc6a-f8e16aabb7de-kube-api-access-sqt5f\") pod \"karsajobs-c79c8864b-kbp7n\" (UID: \"36a0fce7-0d02-41e0-bc6a-f8e16aabb7de\") " pod="default/karsajobs-c79c8864b-kbp7n"
Aug 29 04:31:44 minikube kubelet[2185]: I0829 04:31:44.241209    2185 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/karsajobs-c79c8864b-kbp7n" podStartSLOduration=1930.360605503 podCreationTimestamp="2023-08-29 03:59:31 +0000 UTC" firstStartedPulling="2023-08-29 03:59:32.587130541 +0000 UTC m=+7576.267095761" lastFinishedPulling="2023-08-29 03:59:35.462782962 +0000 UTC m=+7579.142748185" observedRunningTime="2023-08-29 03:59:35.947565609 +0000 UTC m=+7579.627530831" watchObservedRunningTime="2023-08-29 04:31:44.236257927 +0000 UTC m=+9507.916223153"
Aug 29 04:31:44 minikube kubelet[2185]: I0829 04:31:44.249406    2185 topology_manager.go:212] "Topology Admit Handler"
Aug 29 04:31:44 minikube kubelet[2185]: I0829 04:31:44.508504    2185 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-z7wp7\" (UniqueName: \"kubernetes.io/projected/496d705a-21e2-4ba3-ba71-0fed22b6fe2d-kube-api-access-z7wp7\") pod \"karsajobs-ui-54bc8cb55b-gflk8\" (UID: \"496d705a-21e2-4ba3-ba71-0fed22b6fe2d\") " pod="default/karsajobs-ui-54bc8cb55b-gflk8"
Aug 29 04:31:45 minikube kubelet[2185]: I0829 04:31:45.962539    2185 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d1ccd943523f94537a5407b534da375143806f471976486988fb451509c4b8db"
Aug 29 04:32:45 minikube kubelet[2185]: E0829 04:32:45.878811    2185 kubelet.go:2431] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.165s"
Aug 29 04:33:50 minikube kubelet[2185]: E0829 04:33:50.692860    2185 kubelet.go:2431] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.858s"

* 
* ==> kubernetes-dashboard [e6f91830e1a6] <==
* 2023/08/29 04:37:11 [2023-08-29T04:37:11Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Incoming HTTP/1.1 GET /api/v1/systembanner request from 127.0.0.1: 
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:12 Getting list of namespaces
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/29 04:37:12 Getting list of all pods in the cluster
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:12 Getting pod metrics
2023/08/29 04:37:12 [2023-08-29T04:37:12Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:17 [2023-08-29T04:37:17Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:17 Getting list of namespaces
2023/08/29 04:37:17 [2023-08-29T04:37:17Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:17 [2023-08-29T04:37:17Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/29 04:37:17 Getting list of all pods in the cluster
2023/08/29 04:37:17 Getting pod metrics
2023/08/29 04:37:17 [2023-08-29T04:37:17Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:22 [2023-08-29T04:37:22Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/29 04:37:22 Getting list of all pods in the cluster
2023/08/29 04:37:22 [2023-08-29T04:37:22Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:22 Getting list of namespaces
2023/08/29 04:37:22 [2023-08-29T04:37:22Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:22 Getting pod metrics
2023/08/29 04:37:22 [2023-08-29T04:37:22Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:23 [2023-08-29T04:37:23Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:23 Getting list of namespaces
2023/08/29 04:37:23 [2023-08-29T04:37:23Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/29 04:37:23 Getting list of all pods in the cluster
2023/08/29 04:37:23 [2023-08-29T04:37:23Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:23 Getting pod metrics
2023/08/29 04:37:23 [2023-08-29T04:37:23Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:28 [2023-08-29T04:37:28Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:28 Getting list of namespaces
2023/08/29 04:37:28 [2023-08-29T04:37:28Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/29 04:37:28 Getting list of all pods in the cluster
2023/08/29 04:37:28 [2023-08-29T04:37:28Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:28 Getting pod metrics
2023/08/29 04:37:28 [2023-08-29T04:37:28Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:29 [2023-08-29T04:37:29Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/08/29 04:37:29 [2023-08-29T04:37:29Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:29 [2023-08-29T04:37:29Z] Incoming HTTP/1.1 GET /api/v1/log/source/default/karsajobs-c79c8864b-kbp7n/pod request from 127.0.0.1: 
2023/08/29 04:37:30 [2023-08-29T04:37:30Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:30 [2023-08-29T04:37:30Z] Incoming HTTP/1.1 GET /api/v1/log/default/karsajobs-c79c8864b-kbp7n/karsajobs request from 127.0.0.1: 
2023/08/29 04:37:30 [2023-08-29T04:37:30Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/08/29 04:37:30 [2023-08-29T04:37:30Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:30 [2023-08-29T04:37:30Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:33 [2023-08-29T04:37:33Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:33 Getting list of namespaces
2023/08/29 04:37:33 [2023-08-29T04:37:33Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:38 [2023-08-29T04:37:38Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:38 Getting list of namespaces
2023/08/29 04:37:38 [2023-08-29T04:37:38Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:40 [2023-08-29T04:37:40Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:40 Getting list of namespaces
2023/08/29 04:37:40 [2023-08-29T04:37:40Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/29 04:37:41 [2023-08-29T04:37:41Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/29 04:37:41 Getting list of namespaces
2023/08/29 04:37:41 [2023-08-29T04:37:41Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [0e064a9d595f] <==
* I0829 01:53:28.894986       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0829 01:53:58.901655       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [18433731cced] <==
* I0829 02:05:23.900175       1 controller.go:1332] provision "default/mongo-pvc" class "standard": started
I0829 02:05:23.916622       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc", UID:"c81e095f-e983-45cc-8bec-a4ed04d4093d", APIVersion:"v1", ResourceVersion:"1080", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-pvc"
I0829 02:05:23.907873       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    b94d1386-d2a9-47f9-bcb4-6d92f3b8cd07 269 0 2023-08-29 01:53:22 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2023-08-29 01:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d &PersistentVolumeClaim{ObjectMeta:{mongo-pvc  default  c81e095f-e983-45cc-8bec-a4ed04d4093d 1080 0 2023-08-29 02:05:23 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"mongo-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"5Gi"}},"storageClassName":"standard"}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2023-08-29 02:05:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2023-08-29 02:05:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{5368709120 0} {<nil>} 5Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-pvc
I0829 02:05:23.933034       1 controller.go:1439] provision "default/mongo-pvc" class "standard": volume "pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d" provisioned
I0829 02:05:23.933275       1 controller.go:1456] provision "default/mongo-pvc" class "standard": succeeded
I0829 02:05:23.933348       1 volume_store.go:212] Trying to save persistentvolume "pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d"
I0829 02:05:23.944875       1 volume_store.go:219] persistentvolume "pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d" saved
I0829 02:05:23.945830       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc", UID:"c81e095f-e983-45cc-8bec-a4ed04d4093d", APIVersion:"v1", ResourceVersion:"1080", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d
I0829 02:05:23.953016       1 controller.go:1472] delete "pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d": started
I0829 02:05:23.954989       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d    cdb66830-5f64-496b-9f98-17efa806993e 1089 0 2023-08-29 02:05:23 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:8093f75b-992b-42c8-ae70-2a22d1bc562b pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{kube-controller-manager Update v1 2023-08-29 02:05:23 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}} {storage-provisioner Update v1 2023-08-29 02:05:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{5368709120 0} {<nil>} 5Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/mongo-pvc,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:mongo-pvc,UID:c81e095f-e983-45cc-8bec-a4ed04d4093d,APIVersion:v1,ResourceVersion:1080,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0829 02:05:23.958179       1 controller.go:1487] delete "pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d": volume deleted
I0829 02:05:23.969090       1 controller.go:1537] delete "pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d": persistentvolume deleted
I0829 02:05:23.969253       1 controller.go:1542] delete "pvc-c81e095f-e983-45cc-8bec-a4ed04d4093d": succeeded
I0829 02:05:34.651784       1 controller.go:1332] provision "default/mongo-data-mongodb-statefulset-0" class "standard": started
I0829 02:05:34.651911       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    b94d1386-d2a9-47f9-bcb4-6d92f3b8cd07 269 0 2023-08-29 01:53:22 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2023-08-29 01:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-56c51ac6-8cc6-4e23-8993-3a1e67d53f21 &PersistentVolumeClaim{ObjectMeta:{mongo-data-mongodb-statefulset-0  default  56c51ac6-8cc6-4e23-8993-3a1e67d53f21 1108 0 2023-08-29 02:05:34 +0000 UTC <nil> <nil> map[app:mongodb] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2023-08-29 02:05:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{5368709120 0} {<nil>} 5Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-data-mongodb-statefulset-0
I0829 02:05:34.652523       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-data-mongodb-statefulset-0", UID:"56c51ac6-8cc6-4e23-8993-3a1e67d53f21", APIVersion:"v1", ResourceVersion:"1108", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-data-mongodb-statefulset-0"
I0829 02:05:34.652543       1 controller.go:1439] provision "default/mongo-data-mongodb-statefulset-0" class "standard": volume "pvc-56c51ac6-8cc6-4e23-8993-3a1e67d53f21" provisioned
I0829 02:05:34.652858       1 controller.go:1456] provision "default/mongo-data-mongodb-statefulset-0" class "standard": succeeded
I0829 02:05:34.657705       1 volume_store.go:212] Trying to save persistentvolume "pvc-56c51ac6-8cc6-4e23-8993-3a1e67d53f21"
I0829 02:05:34.671965       1 volume_store.go:219] persistentvolume "pvc-56c51ac6-8cc6-4e23-8993-3a1e67d53f21" saved
I0829 02:05:34.672161       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-data-mongodb-statefulset-0", UID:"56c51ac6-8cc6-4e23-8993-3a1e67d53f21", APIVersion:"v1", ResourceVersion:"1108", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-56c51ac6-8cc6-4e23-8993-3a1e67d53f21
I0829 02:07:01.754506       1 controller.go:1332] provision "default/mongo-data-mongodb-statefulset-1" class "standard": started
I0829 02:07:01.755407       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    b94d1386-d2a9-47f9-bcb4-6d92f3b8cd07 269 0 2023-08-29 01:53:22 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2023-08-29 01:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-b4047dc7-78e2-4587-aacb-e2b30761b5c7 &PersistentVolumeClaim{ObjectMeta:{mongo-data-mongodb-statefulset-1  default  b4047dc7-78e2-4587-aacb-e2b30761b5c7 1203 0 2023-08-29 02:07:01 +0000 UTC <nil> <nil> map[app:mongodb] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2023-08-29 02:07:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{5368709120 0} {<nil>} 5Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-data-mongodb-statefulset-1
I0829 02:07:01.756552       1 controller.go:1439] provision "default/mongo-data-mongodb-statefulset-1" class "standard": volume "pvc-b4047dc7-78e2-4587-aacb-e2b30761b5c7" provisioned
I0829 02:07:01.756705       1 controller.go:1456] provision "default/mongo-data-mongodb-statefulset-1" class "standard": succeeded
I0829 02:07:01.757050       1 volume_store.go:212] Trying to save persistentvolume "pvc-b4047dc7-78e2-4587-aacb-e2b30761b5c7"
I0829 02:07:01.758104       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-data-mongodb-statefulset-1", UID:"b4047dc7-78e2-4587-aacb-e2b30761b5c7", APIVersion:"v1", ResourceVersion:"1203", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-data-mongodb-statefulset-1"
I0829 02:07:01.768178       1 volume_store.go:219] persistentvolume "pvc-b4047dc7-78e2-4587-aacb-e2b30761b5c7" saved
I0829 02:07:01.768775       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-data-mongodb-statefulset-1", UID:"b4047dc7-78e2-4587-aacb-e2b30761b5c7", APIVersion:"v1", ResourceVersion:"1203", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-b4047dc7-78e2-4587-aacb-e2b30761b5c7
I0829 02:07:08.721848       1 controller.go:1332] provision "default/mongo-data-mongodb-statefulset-2" class "standard": started
I0829 02:07:08.727832       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-data-mongodb-statefulset-2", UID:"7b9ce62c-1a86-4169-a111-9d2004de3bb7", APIVersion:"v1", ResourceVersion:"1235", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-data-mongodb-statefulset-2"
I0829 02:07:08.721928       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    b94d1386-d2a9-47f9-bcb4-6d92f3b8cd07 269 0 2023-08-29 01:53:22 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2023-08-29 01:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-7b9ce62c-1a86-4169-a111-9d2004de3bb7 &PersistentVolumeClaim{ObjectMeta:{mongo-data-mongodb-statefulset-2  default  7b9ce62c-1a86-4169-a111-9d2004de3bb7 1235 0 2023-08-29 02:07:08 +0000 UTC <nil> <nil> map[app:mongodb] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2023-08-29 02:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{5368709120 0} {<nil>} 5Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-data-mongodb-statefulset-2
I0829 02:07:08.733282       1 controller.go:1439] provision "default/mongo-data-mongodb-statefulset-2" class "standard": volume "pvc-7b9ce62c-1a86-4169-a111-9d2004de3bb7" provisioned
I0829 02:07:08.733415       1 controller.go:1456] provision "default/mongo-data-mongodb-statefulset-2" class "standard": succeeded
I0829 02:07:08.733427       1 volume_store.go:212] Trying to save persistentvolume "pvc-7b9ce62c-1a86-4169-a111-9d2004de3bb7"
I0829 02:07:08.750021       1 volume_store.go:219] persistentvolume "pvc-7b9ce62c-1a86-4169-a111-9d2004de3bb7" saved
I0829 02:07:08.756198       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-data-mongodb-statefulset-2", UID:"7b9ce62c-1a86-4169-a111-9d2004de3bb7", APIVersion:"v1", ResourceVersion:"1235", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-7b9ce62c-1a86-4169-a111-9d2004de3bb7
I0829 03:56:56.386817       1 controller.go:1332] provision "default/mongo-pvc-mongodb-statefulset-0" class "standard": started
I0829 03:56:56.426888       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc-mongodb-statefulset-0", UID:"e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7", APIVersion:"v1", ResourceVersion:"6818", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-pvc-mongodb-statefulset-0"
I0829 03:56:56.426918       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    b94d1386-d2a9-47f9-bcb4-6d92f3b8cd07 269 0 2023-08-29 01:53:22 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2023-08-29 01:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7 &PersistentVolumeClaim{ObjectMeta:{mongo-pvc-mongodb-statefulset-0  default  e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7 6818 0 2023-08-29 03:56:56 +0000 UTC <nil> <nil> map[app:mongodb] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2023-08-29 03:56:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{5368709120 0} {<nil>} 5Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-pvc-mongodb-statefulset-0
I0829 03:56:56.457902       1 controller.go:1439] provision "default/mongo-pvc-mongodb-statefulset-0" class "standard": volume "pvc-e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7" provisioned
I0829 03:56:56.458042       1 controller.go:1456] provision "default/mongo-pvc-mongodb-statefulset-0" class "standard": succeeded
I0829 03:56:56.458338       1 volume_store.go:212] Trying to save persistentvolume "pvc-e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7"
I0829 03:56:56.488376       1 volume_store.go:219] persistentvolume "pvc-e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7" saved
I0829 03:56:56.491549       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc-mongodb-statefulset-0", UID:"e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7", APIVersion:"v1", ResourceVersion:"6818", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-e43de9e5-c3d6-4b2d-abd8-04dcd52e5de7
I0829 03:57:03.590664       1 controller.go:1332] provision "default/mongo-pvc-mongodb-statefulset-1" class "standard": started
I0829 03:57:03.590958       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    b94d1386-d2a9-47f9-bcb4-6d92f3b8cd07 269 0 2023-08-29 01:53:22 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2023-08-29 01:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-f5ca182c-8512-40e6-9b08-00cc20a3bbdc &PersistentVolumeClaim{ObjectMeta:{mongo-pvc-mongodb-statefulset-1  default  f5ca182c-8512-40e6-9b08-00cc20a3bbdc 6853 0 2023-08-29 03:57:03 +0000 UTC <nil> <nil> map[app:mongodb] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2023-08-29 03:57:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{5368709120 0} {<nil>} 5Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-pvc-mongodb-statefulset-1
I0829 03:57:03.592996       1 controller.go:1439] provision "default/mongo-pvc-mongodb-statefulset-1" class "standard": volume "pvc-f5ca182c-8512-40e6-9b08-00cc20a3bbdc" provisioned
I0829 03:57:03.593069       1 controller.go:1456] provision "default/mongo-pvc-mongodb-statefulset-1" class "standard": succeeded
I0829 03:57:03.593131       1 volume_store.go:212] Trying to save persistentvolume "pvc-f5ca182c-8512-40e6-9b08-00cc20a3bbdc"
I0829 03:57:03.606244       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc-mongodb-statefulset-1", UID:"f5ca182c-8512-40e6-9b08-00cc20a3bbdc", APIVersion:"v1", ResourceVersion:"6853", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-pvc-mongodb-statefulset-1"
I0829 03:57:03.613326       1 volume_store.go:219] persistentvolume "pvc-f5ca182c-8512-40e6-9b08-00cc20a3bbdc" saved
I0829 03:57:03.613850       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc-mongodb-statefulset-1", UID:"f5ca182c-8512-40e6-9b08-00cc20a3bbdc", APIVersion:"v1", ResourceVersion:"6853", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-f5ca182c-8512-40e6-9b08-00cc20a3bbdc

